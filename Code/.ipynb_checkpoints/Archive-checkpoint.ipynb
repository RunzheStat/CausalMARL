{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy is valid under this setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_compare(u_O, w_A = 1, w_O = 0.2, details = False):\n",
    "    def one_threshold(threshold):\n",
    "        print(\"Threshold:\", threshold, \"\\n\")\n",
    "        target_policy = simu_target_policy_pattern(u_O = u_O, threshold = threshold)\n",
    "        \n",
    "        aa = []\n",
    "        l = 5\n",
    "        O = [[] for i in range(l**2)]\n",
    "        D = [[] for i in range(l**2)]\n",
    "        M  = [[] for i in range(l**2)]\n",
    "        for seed in range(5):\n",
    "            r = DG_once(seed = seed, l = 5, T = 14 * 48, time_dependent = False, w_A = w_A, w_O = w_O, sd_R  = 1e-5, sd_D = 3, u_O = u_O, \n",
    "                   TARGET = True, target_policy = target_policy, T_burn_in = 50,  dynamics = \"old\")\n",
    "            a = np.mean(r[2][2])\n",
    "            data = r[0]\n",
    "            for i in range(l**2):\n",
    "                data_i = data[i]\n",
    "                O[i].append(np.mean([b[0][0] for b in data_i]))\n",
    "                D[i].append(np.mean([b[0][1] for b in data_i]))\n",
    "                M[i].append(np.mean([b[0][2] for b in data_i]))\n",
    "            aa.append(a)\n",
    "            \n",
    "        if details:\n",
    "            print(DASH, \"means of Order:\")\n",
    "            for i in range(l):\n",
    "                for j in range(l):\n",
    "                    print(round(np.mean(O[i * l + j]),3), end = \" \")\n",
    "                print(\"\\n\")\n",
    "\n",
    "            print(DASH, \"means of Driver:\")\n",
    "            for i in range(l):\n",
    "                for j in range(l):\n",
    "                    print(round(np.mean(D[i * l + j]),3), end = \" \")\n",
    "                print(\"\\n\")\n",
    "\n",
    "            print(DASH, \"means of Mismatch:\")\n",
    "            for i in range(l):\n",
    "                for j in range(l):\n",
    "                    print(round(np.mean(M[i * l + j]),3), end = \" \")\n",
    "                print(\"\\n\")\n",
    "        \n",
    "        print(\"value:\", np.mean(aa), np.std(aa))\n",
    "\n",
    "        print(DASH)\n",
    "    one_threshold(12) # a good policy\n",
    "    one_threshold(120) # no reward\n",
    "    a = DG_once(seed = 0, l = 5, T = 14 * 48, time_dependent = False, w_A = w_A, w_O = w_O, sd_R  = 1e-5, sd_D = 3, u_O = u_O, \n",
    "                   TARGET = False, target_policy = None, T_burn_in = 50, dynamics = \"old\")[2][2]\n",
    "    print(np.mean(a)) # random reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 12 \n",
      "\n",
      "target policy: \n",
      "\n",
      "0 0 0 0 0 \n",
      "\n",
      "0 1 0 1 0 \n",
      "\n",
      "0 0 1 0 0 \n",
      "\n",
      "0 1 0 1 0 \n",
      "\n",
      "0 0 0 0 0 \n",
      "\n",
      "means of Order: \n",
      "\n",
      "6 6 6 6 6 \n",
      "\n",
      "6 18 6 18 6 \n",
      "\n",
      "6 6 24 6 6 \n",
      "\n",
      "6 18 6 18 6 \n",
      "\n",
      "6 6 6 6 6 \n",
      "\n",
      "value: 5.670819238727093 0.016559976076961625\n",
      "\n",
      "--------------------------------------\n",
      "\n",
      "Threshold: 120 \n",
      "\n",
      "target policy: \n",
      "\n",
      "0 0 0 0 0 \n",
      "\n",
      "0 0 0 0 0 \n",
      "\n",
      "0 0 0 0 0 \n",
      "\n",
      "0 0 0 0 0 \n",
      "\n",
      "0 0 0 0 0 \n",
      "\n",
      "means of Order: \n",
      "\n",
      "6 6 6 6 6 \n",
      "\n",
      "6 18 6 18 6 \n",
      "\n",
      "6 6 24 6 6 \n",
      "\n",
      "6 18 6 18 6 \n",
      "\n",
      "6 6 6 6 6 \n",
      "\n",
      "value: 4.851261692732304 0.02180556159273956\n",
      "\n",
      "--------------------------------------\n",
      "\n",
      "4.619850845996925\n"
     ]
    }
   ],
   "source": [
    "u_O = [[6 for j in range(5)] for i in range(5)]\n",
    "u_O[1][1] = u_O[1][3] = u_O[3][1] = u_O[3][3] = 18\n",
    "u_O[2][2] = 24\n",
    "a = u_O[0]\n",
    "for i in range(1,5):\n",
    "    a += u_O[i]\n",
    "u_O = a\n",
    "\n",
    "# npseed(1)\n",
    "# u_O = rlogN(2.5, .2, 5**2) \n",
    "target_compare(u_O, w_O = 0.05)\n",
    "\n",
    "# print(DASH, \"NEXT\", DASH)\n",
    "\n",
    "# u_O = [[10 for j in range(5)] for i in range(5)]\n",
    "# u_O[1][1] = u_O[1][3] = u_O[3][1] = u_O[3][3] = 14\n",
    "# u_O[2][2] = 18\n",
    "# a = u_O[0]\n",
    "# for i in range(1,5):\n",
    "#     a += u_O[i]\n",
    "# u_O = a\n",
    "\n",
    "# # npseed(1)\n",
    "# # u_O = rlogN(2.5, .2, 5**2) \n",
    "# target_compare(u_O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpler version of ours\n",
    "1. both with or without M_t works well with QV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utility import *\n",
    "from weight import *\n",
    "from simu_funs import *\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DG_once(seed = 1, T = 14 * 48 + 50, w_A = 8, w_O = 1, sd_R  = 1, sd_D = 1, \n",
    "            u_O = 12, p_behav = 0.5,\n",
    "           TARGET = False, T_burn_in = 50):  \n",
    "    \"\"\" prepare data\n",
    "    \"\"\"\n",
    "    T = T + T_burn_in\n",
    "    npseed(seed)\n",
    "    w_M = 0.8 \n",
    "    \n",
    "    \"\"\" O: the pattern (spatial distribution) of orders\n",
    "    \"\"\"\n",
    "#     O = rpoisson(u_O, (T))\n",
    "    O = u_O + randn(T) / 10\n",
    "    \n",
    "    \"\"\" D: initial is the same with driver. then attract by the A and O. burn-in.\n",
    "    \"\"\"\n",
    "    u_D = np.mean(u_O - 6)\n",
    "    D = [u_D]\n",
    "    \n",
    "    \"\"\" Actions\n",
    "    \"\"\"\n",
    "    if TARGET: # target. fixed. \n",
    "        A = rbin(1, 1, (T))\n",
    "    else: # behaviour policy: random\n",
    "        A = rbin(1, 0.5, (T))\n",
    "    \n",
    "\n",
    "    e_D = (rpoisson(1, (T)) - 1) * 0.01\n",
    "\n",
    "    \"\"\" Initialization\n",
    "    \"\"\"\n",
    "    M = [0.5] \n",
    "    R = []\n",
    "    \n",
    "    \"\"\" MAIN: state trasition and reward calculation [no action selection]\n",
    "    \"\"\"\n",
    "    for t in range(1, T): \n",
    "        \"\"\" Drivers\n",
    "        \"\"\"\n",
    "        ## attractions\n",
    "        D_t = (u_D + D[t - 1] + w_A * A[t-1] ) / 2 \n",
    "        \n",
    "        D.append(D_t)\n",
    "        \"\"\" New Order and Mismatch\n",
    "        \"\"\"\n",
    "        O_t = O[t]\n",
    "        M_t = w_M * (1 - abs(D_t - O_t) / abs(1 + D_t + O_t)) + (1 - w_M) * M[t - 1]\n",
    "        M.append(M_t)\n",
    "        \n",
    "        \"\"\" Reward definitions\n",
    "        \"\"\"\n",
    "#         R_t_1 = np.minimum(D_t, O_t)  \n",
    "        R_t_1 = np.minimum(D_t, O_t) * M_t\n",
    "        R.append(R_t_1)\n",
    "    R.append(R_t_1) # add one more? # new reward?\n",
    "    \n",
    "    ## organization and burn-in; N * T\n",
    "    R = arr(R).T[T_burn_in:]; D = arr(D).T[T_burn_in:]; M = arr(M).T[T_burn_in:]\n",
    "    O = O[T_burn_in:]; A = A[T_burn_in:]\n",
    "        \n",
    "    \"\"\" reorganization\n",
    "    \"\"\"\n",
    "    data_i = []\n",
    "    for t in range(T - T_burn_in):\n",
    "        data_i.append([arr([O[t], D[t], M[t]]), A[t], R[t]])\n",
    "    return data_i, [[O, D, M], A, R]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRegionData(data_i):\n",
    "    T = len(data_i) - 1\n",
    "    tuples_i = []\n",
    "    pi_Sit_1s = rbin(1, 1, (T)) # TARGET\n",
    "    for t in range(T):\n",
    "        tuple_t = data_i[t].copy()\n",
    "        S_it1 = data_i[t + 1][0]\n",
    "        A_it1 = data_i[t + 1][1]\n",
    "        pi_Sit_1 = pi_Sit_1s[t]\n",
    "        tuple_t += [S_it1, A_it1, pi_Sit_1]\n",
    "        tuples_i.append(tuple_t)\n",
    "    return tuples_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeQV_one(tuples_i, R, penalty):\n",
    "    \n",
    "    ## prepare data\n",
    "    R = arr([a[2] for a in tuples_i]) # began to use Rit\n",
    "    T = len(tuples_i)\n",
    "    mu, lam = penalty\n",
    "    A_set = set([a[1] for a in tuples_i])\n",
    "    \n",
    "    ## get (S,A) pair\n",
    "\n",
    "    Z = np.array([np.concatenate((a[0], [a[1]])) for a in tuples_i]) # T * p. [S, A]\n",
    "    Zstar = np.array([np.concatenate((a[3], [a[5]])) for a in tuples_i])\n",
    "\n",
    "    Z_tilde = np.vstack((Z, Zstar))\n",
    "    \n",
    "    ## kernel distance\n",
    "    \n",
    "    def SA_GRBF(Z, gamma):\n",
    "        l = Z.shape[1] - 1\n",
    "        T = Z.shape[0]\n",
    "        A = Z[:, -1]\n",
    "        A_mat = (A.reshape(-1,1) == A.reshape(1,-1))\n",
    "        \n",
    "        K = GRBF(Z[:,:(l-1)], Z[:,:(l-1)], gamma) + identity(T) * 1e-8\n",
    "        return np.multiply(K, A_mat)\n",
    "    \n",
    "    gamma_g = 1 / (2 * np.median(pdist(Z[:,:(Z.shape[1]-1)]))**2)\n",
    "    gamma_q = 1 / (2 * np.median(pdist(Z_tilde[:,:(Z_tilde.shape[1]-1)]))**2)\n",
    "    Kg = SA_GRBF(Z, gamma_g)\n",
    "    KQ = SA_GRBF(Z_tilde, gamma_q)\n",
    "    # centeralization, p11\n",
    "    ZTstar = np.mean(Z_tilde, 0)\n",
    "    \n",
    "    KQ = KQ - GRBF(Z_tilde, ZTstar.reshape(1, -1), gamma_q) -  GRBF(ZTstar.reshape(1, -1), Z_tilde, gamma_q) - GRBF(ZTstar.reshape(1, -1), ZTstar.reshape(1, -1), gamma_q)[0][0]\n",
    "    \n",
    "    ## Idnetity vec/mat\n",
    "    C = np.hstack((-identity(T),identity(T)))       \n",
    "    vec1, I = ones(T).reshape(-1,1), identity(T)\n",
    "    \n",
    "    E_right_bef_inverse = Kg + T * mu * I # RHS of E\n",
    "    \n",
    "    CKQ_1 = np.hstack((C.dot(KQ), -vec1))\n",
    "    ECKQ1 = Kg.T.dot(solve(E_right_bef_inverse, CKQ_1)) # E[CK_Q,-1]\n",
    "    \n",
    "    left = (ECKQ1.T.dot(ECKQ1) + np.vstack((np.hstack((T * lam * KQ, zeros((T * 2, 1)))), zeros((1, T * 2 + 1))))) # Left part of (\\hat{\\alpha}, \\hat{\\eta})\n",
    "    \n",
    "    right = ECKQ1.T.dot(Kg.dot(solve(E_right_bef_inverse, R))) # Right part of (\\hat{\\alpha}, \\hat{\\eta})\n",
    "    alpha_eta = -solve(left, np.expand_dims(right,1)) \n",
    "    alpha = alpha_eta[:(len(alpha_eta) - 1)]\n",
    "    Vi = eta = alpha_eta[-1]\n",
    "    \n",
    "    \"\"\" no centeralization for KQ [? for below?]\n",
    "    \"\"\"\n",
    "\n",
    "    Qvalues = alpha.T.dot(KQ)\n",
    "    Qi_diff = Qvalues[0, T:] - Qvalues[0, :T] # Q^* - Q\n",
    "         \n",
    "    return Qi_diff, Vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN works well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy0(S):\n",
    "    return 0.5\n",
    "def policy1(S):\n",
    "    return 1\n",
    "\n",
    "# weight 里面的 retio 也需要改\n",
    "# [S_it, A_it, R_it, Tsit, Tait, S_i(t+1), Tsi(t+1), pi_Sit_1, T_ait_1_pi]\n",
    "\n",
    "# tuples_i[t] = [S_it, A_it, R_it, Tsit, Tait,  # 0 - 4\n",
    "#                     S_i(t+1), Tsi(t+1), pi_Sit_1, T_ait_1_pi] # 5 - 8\n",
    "\n",
    "def getWeight(tuples_i, policy0, policy1, dim_S_plus_Ts, \n",
    "              w_hidden = 10, Learning_rate = 1e-4,  n_layer = 2, \n",
    "              batch_size = 64, max_iteration = 1001, test_num = 0, epsilon = 1e-3,\n",
    "             spatial = False, isValidation = False):\n",
    "    # prepare transition pairs\n",
    "    reg_weight = 0 # no penalty in the two-layer NN\n",
    "    def concateOne(tuplet):\n",
    "            return [tuplet[0], tuplet[1],\n",
    "                    tuplet[3], tuplet[2]]\n",
    "    SASR_i = [concateOne(tuplet) for tuplet in tuples_i]\n",
    "    \n",
    "    SASR_i = [SASR_i] # although we only need 1 layer of list\n",
    "        \n",
    "    obs_dim = int(dim_S_plus_Ts / 2)\n",
    "    computeWeight = Density_Ratio_kernel(obs_dim = obs_dim, n_layer = n_layer, \n",
    "                                 w_hidden = w_hidden, Learning_rate = Learning_rate, reg_weight = reg_weight)\n",
    "    \n",
    "    weights = computeWeight.train(SASR_i, policy0, policy1, \n",
    "              batch_size = batch_size, max_iteration = max_iteration, n_neigh = None, \n",
    "              test_num = test_num, epsilon = epsilon, only_state = False, spatial = False)\n",
    "    computeWeight.close_Session()\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.530979882392066\n",
      "10.938511933279807\n",
      "9.92381228219302\n",
      "10.266099238194366\n",
      "9.507713579235602\n",
      "9.51745950205572\n",
      "9.733662098366823\n",
      "9.71867060213315\n",
      "10.660901622605534\n",
      "10.382218101797296\n",
      "10.218002884225339 0.6351550865012732\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "for seed in range(10):\n",
    "    data_i, details = DG_once(TARGET = False, seed = seed, T = 1000)\n",
    "    tuples_i = getRegionData(data_i)\n",
    "    R = details[2]\n",
    "    R = R[:(len(R) - 1)]\n",
    "    w = getWeight(tuples_i = tuples_i, policy0 = policy0, policy1 = policy1, dim_S_plus_Ts = 3 + 3)\n",
    "    value = np.mean(w * R)\n",
    "    print(value)\n",
    "    values.append(value)\n",
    "print(np.mean(values), np.std(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: 11.111894448616392 0.007121934698886889\n",
      "Behav: 8.827027627284124 0.14494684990663562\n"
     ]
    }
   ],
   "source": [
    "V_MC = []\n",
    "for i in range(10):\n",
    "    V_MC.append(np.mean(DG_once(TARGET = True, seed = i)[1][2]))\n",
    "print(\"TARGET:\",np.mean(V_MC), np.std(V_MC))\n",
    "\n",
    "V_MC = []\n",
    "for i in range(10):\n",
    "    V_MC.append(np.mean(DG_once(TARGET = False, seed = i)[1][2]))\n",
    "print(\"Behav:\",np.mean(V_MC), np.std(V_MC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Susan's setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utility import *\n",
    "from weight import *\n",
    "from simu_funs import *\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DG_once_susan(seed = 1, T = 50 , N = 25, p = 0.5): \n",
    "    npseed(seed)\n",
    "    for i in range(N):\n",
    "        states = [randn(2)] \n",
    "        e1 = randn(T) / 4\n",
    "        e2 = randn(T) / 4\n",
    "        A = rbin(1, p, T).tolist()\n",
    "        R = []\n",
    "        for t in range(1,T):\n",
    "            S1, S2 = states[t - 1]\n",
    "            S11 = 3/4 * (2 * A[t-1] - 1 ) * S1 + 1 / 4 * S1 * S2 + e1[t]\n",
    "            S22 = 3/4 * (1 - 2 * A[t-1] ) * S2 + 1 / 4 * S1 * S2 + e1[t]\n",
    "            states.append([S11, S22])\n",
    "            R.append(S11 + S22 / 2 + (2 * A[t-1]) / 4)\n",
    "        R.append(0)\n",
    "        if i == 0:\n",
    "            As = A\n",
    "            Rs = R\n",
    "            statess = states\n",
    "        else:\n",
    "            As = As + A\n",
    "            Rs = Rs + R\n",
    "            statess = statess + states\n",
    "    ## organization and burn-in; N * T\n",
    "#     R = arr(R)\n",
    "#     S = \n",
    "        \n",
    "    \"\"\" reorganization\n",
    "    \"\"\"\n",
    "    data_i = []\n",
    "    for t in range((T - 1) * N):\n",
    "        data_i.append([statess[t], As[t], Rs[t]])   \n",
    "    return data_i, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRegionData_susan(data_i, pi):\n",
    "    T = len(data_i) - 1\n",
    "    tuples_i = []\n",
    "    pi_Sit_1s = rbin(1, pi, (T))\n",
    "    for t in range(T):\n",
    "        tuple_t = data_i[t].copy()\n",
    "        S_it1 = data_i[t + 1][0]\n",
    "        A_it1 = data_i[t + 1][1]\n",
    "        pi_Sit_1 = pi_Sit_1s[t] # pi(S_{t+1})\n",
    "        tuple_t += [S_it1, A_it1, pi_Sit_1]\n",
    "        tuples_i.append(tuple_t)\n",
    "    return tuples_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeQV_one(tuples_i, R, penalty):\n",
    "    \n",
    "    ## prepare data\n",
    "    R = arr([a[2] for a in tuples_i]) # began to use Rit\n",
    "    T = len(tuples_i)\n",
    "    mu, lam = penalty\n",
    "    A_set = set([a[1] for a in tuples_i])\n",
    "    \n",
    "    ## get (S,A) pair\n",
    "\n",
    "    Z = np.array([np.concatenate((a[0], [a[1]])) for a in tuples_i]) # T * p. [S, A]\n",
    "    Zstar = np.array([np.concatenate((a[3], [a[5]])) for a in tuples_i])\n",
    "\n",
    "    \n",
    "    Z_tilde = np.vstack((Z, Zstar))\n",
    "    \n",
    "    ## kernel distance\n",
    "    \n",
    "    def SA_GRBF(Z, gamma):\n",
    "        l = Z.shape[1] - 1\n",
    "        T = Z.shape[0]\n",
    "        A = Z[:, -1]\n",
    "        A_mat = (A.reshape(-1,1) == A.reshape(1,-1))\n",
    "        \n",
    "        K = GRBF(Z[:,:(l-1)], Z[:,:(l-1)], gamma) + identity(T) * 1e-8\n",
    "        return np.multiply(K, A_mat)\n",
    "    \n",
    "    gamma_g = 1 / (2 * np.median(pdist(Z[:,:(Z.shape[1]-1)]))**2)\n",
    "    gamma_q = 1 / (2 * np.median(pdist(Z_tilde[:,:(Z_tilde.shape[1]-1)]))**2)\n",
    "    Kg = SA_GRBF(Z, gamma_g)\n",
    "    KQ = SA_GRBF(Z_tilde, gamma_q)\n",
    "    # centeralization, p11\n",
    "    ZTstar = np.mean(Z_tilde, 0)\n",
    "    \n",
    "    KQ = KQ - GRBF(Z_tilde, ZTstar.reshape(1, -1), gamma_q) -  GRBF(ZTstar.reshape(1, -1), Z_tilde, gamma_q) - GRBF(ZTstar.reshape(1, -1), ZTstar.reshape(1, -1), gamma_q)[0][0]\n",
    "    \n",
    "    ## Idnetity vec/mat\n",
    "    C = np.hstack((-identity(T),identity(T)))       \n",
    "    vec1, I = ones(T).reshape(-1,1), identity(T)\n",
    "    \n",
    "    E_right_bef_inverse = Kg + T * mu * I # RHS of E\n",
    "    \n",
    "    CKQ_1 = np.hstack((C.dot(KQ), -vec1))\n",
    "    ECKQ1 = Kg.T.dot(solve(E_right_bef_inverse, CKQ_1)) # E[CK_Q,-1]\n",
    "    \n",
    "    left = (ECKQ1.T.dot(ECKQ1) + np.vstack((np.hstack((T * lam * KQ, zeros((T * 2, 1)))), zeros((1, T * 2 + 1))))) # Left part of (\\hat{\\alpha}, \\hat{\\eta})\n",
    "    \n",
    "    right = ECKQ1.T.dot(Kg.dot(solve(E_right_bef_inverse, R))) # Right part of (\\hat{\\alpha}, \\hat{\\eta})\n",
    "    alpha_eta = -solve(left, np.expand_dims(right,1)) \n",
    "    alpha = alpha_eta[:(len(alpha_eta) - 1)]\n",
    "    Vi = eta = alpha_eta[-1]\n",
    "    \n",
    "    \"\"\" no centeralization for KQ [? for below?]\n",
    "    \"\"\"\n",
    "\n",
    "    Qvalues = alpha.T.dot(KQ)\n",
    "    Qi_diff = Qvalues[0, T:] - Qvalues[0, :T] # Q^* - Q\n",
    "         \n",
    "    return Qi_diff, Vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for seed in range(3):\n",
    "    data_i, R = DG_once_susan(p = 0.5, seed = seed, T = 100)# , N = 25\n",
    "    tuples_i = getRegionData_susan(data_i, pi = 1)\n",
    "    value = computeQV_one(tuples_i, R, penalty = [0.1, 0.1])[1]\n",
    "    print(value)\n",
    "    values.append(value)\n",
    "print(np.mean(values), np.std(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: 0.2655467109252993 0.013062885144231682\n",
      "T1: 0.5453472021110254 0.02133186123333595\n",
      "T0: 0.02890430693339857 0.01283215587099297\n"
     ]
    }
   ],
   "source": [
    "V_MC = []\n",
    "for seed in range(10):\n",
    "    V_MC.append(np.mean(DG_once_susan(p = 0.5, seed = seed, T = 1000)[1]))\n",
    "print(\"B:\",np.mean(V_MC), np.std(V_MC))\n",
    "\n",
    "V_MC = []\n",
    "for seed in range(10):\n",
    "    V_MC.append(np.mean(DG_once_susan(p = 1, seed = seed, T = 1000)[1]))\n",
    "print(\"T1:\",np.mean(V_MC), np.std(V_MC))\n",
    "\n",
    "\n",
    "V_MC = []\n",
    "for seed in range(10):\n",
    "    V_MC.append(np.mean(DG_once_susan(p = 0, seed = seed, T = 1000)[1]))\n",
    "print(\"T0:\",np.mean(V_MC), np.std(V_MC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
