{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simulation for CausalMARL\n",
    "\n",
    "## TODO\n",
    "1. debug: \n",
    "    1. try DGP with no neigh [identity ADJ]\n",
    "1. Tuning:\n",
    "    1. Two penalties in Susan: their CV method\n",
    "    2. [w_hidden, Learning_rate] of NN: CV\n",
    "    4. Now: manually fix some values\n",
    "    3. **Q-V CV**: hard to choose neighbour actions. we can do that? but very large?\n",
    "5. spatial information - model\n",
    "2. DR w/o mean-field \n",
    "    1. $R_it$: \n",
    "        1. with neigh states/actions as appending\n",
    "        2. with global states/actions as appending \n",
    "    2. $\\bar{R}_t$ + global states and actions\n",
    "    3. multi-dim actions    \n",
    "\n",
    "1. why wi estimation is not good?\n",
    "    1. why R2 close to  0? what is our attempt?\n",
    "    2. density close to zero, den ratio is very large, even after normalization, std is large\n",
    "1. how to evaluate the true density and Q/V functions?\n",
    "2. Check the current model and policy\n",
    "    1. stationary?\n",
    "1. Design a toy example\n",
    "    1. need DR (how?)\n",
    "        1. weight learned with a linear function\n",
    "        2. Q learned wtih a linear function\n",
    "    2. need spatial (mean state and action)\n",
    "        1. R_i = X1 * X2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "刚才的结果:  有 small O and new dynamics\n",
    "\n",
    "现在： 有 large O and old dynamics\n",
    "\n",
    "NN 没有弄好\n",
    "\n",
    "好多不同的setting啊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "希望结果好\n",
    "setting 也好\n",
    "\n",
    "既然是好的\n",
    "就跟春哥说一声\n",
    "并且自己试验一下\n",
    "\n",
    "\n",
    "another suggestion on the setting: could set attraction to be proportional to 1/D_t. This might help the system to achieve stationarity\n",
    "\n",
    "我的新setting\n",
    "\n",
    "结果似乎不算好\n",
    "\n",
    "为何还是large bbias?\n",
    "而且好像还是behaviour好？\n",
    "\n",
    "NN 的 R 不对？\n",
    "\n",
    "我觉得是O 波动 太大了？\n",
    "\n",
    "\n",
    "bias increased so much?\n",
    "\n",
    "没有拿到USA data 啊？\n",
    "investigate 到底哪儿出问题了？\n",
    "\n",
    "昨天做的探究的结果\n",
    "\n",
    "\n",
    "\n",
    "spatial is true or false\n",
    "density == 0\n",
    "discrete\n",
    "why???\n",
    "\n",
    "比较奇怪的是No_MARL的表现反而比较好\n",
    "\n",
    "现在Q估的是准的了是吧\n",
    "\n",
    "我觉得可以调一下weight的hyperparameter?\n",
    "让他overfit一点，reduce bias\n",
    "\n",
    "然后我觉得就试那个 中间5个1那个setting就好了\n",
    "\n",
    "还有一个suggestion，可以看一下把所有Neigbour的state,action加进去，而不只是加mean-field的state和action是不是会好一点？\n",
    "\n",
    "也可以加大sample size看看结果\n",
    "\n",
    "你看看不去都是负的吗\n",
    "\n",
    "在single-agent 时都是work 的 挺好的。\n",
    "还是setting的问题？\n",
    "\n",
    "现在尝试 小一点的波动\n",
    "\n",
    "IS 还不如啥都不用呢\n",
    "\n",
    "\n",
    "\n",
    "代码totally different?\n",
    "\n",
    "fix a setting and discuss the trend with lambda: etc\n",
    "I can add some pattern to D and O. but the pattern needs to be shared between them.\n",
    "why no trend with T? \n",
    "怎么估计QV? sample-spliting?\n",
    "change default values\n",
    "\n",
    "mean reversion 以后 std少了很多\n",
    "但是 driver 应该是被order (and thus reward) 吸引过去的。而不是自身就是mean reversion的?\n",
    "\n",
    "【\n",
    "需要什么用途呢？\n",
    "random policy?\n",
    "需要能 address policy 的弊病】\n",
    "\n",
    "【\n",
    "state, action 和 reward之间要有一定的规律 才能learn something.\n",
    "w: state ratio\n",
    "\n",
    "NN on w and then use total ratio to do sth\n",
    "只需要 s,a, s' 有一定的关系 [but no?]\n",
    "\n",
    "Q,V: Q(s,a) = r 之间要有关系\n",
    "\n",
    "Order, Driver, T_order, T_driver\n",
    "M, T_M\n",
    "Action\n",
    "reward\n",
    "exam the reward: reasonable?\n",
    "\n",
    "】\n",
    "\n",
    "sd_D = 3 before\n",
    "\n",
    "许多可以修改的\n",
    "\n",
    "现在没有noise的时候能估计的准确吧\n",
    "至少要这样\n",
    "\n",
    "over attraction? S, A, R 无关系？\n",
    "\n",
    "这时候做evaluate learn 出来的 Q 和 weight 都丝毫不靠谱。要不就是MC的问题 要不就是啥也没学会 [?]\n",
    "\n",
    "why not just use a good behaviour policy as the behaviour? \n",
    "要不反过来？\n",
    "behaviour is well-behaved, but target is not\n",
    "\n",
    "value 大 = 1 - mismatch 大\n",
    "\n",
    "why M should have temporal correlatioon?\n",
    "\n",
    "mismatch 应该放进入 dynamics?\n",
    "\n",
    "no difference. still large bias\n",
    "\n",
    "给action 真的不一定会弄得更好。\n",
    "为什么要给一些 奖励呢？\n",
    "那是因为 order 本身 不足够吸引对方过去\n",
    "\n",
    "有mismatch才要吸引过去吧\n",
    "\n",
    "所以才要有mean reversion\n",
    "就是某个地方 一直order mismatch等等 \n",
    "就是这个reward实在是不合理啊\n",
    "\n",
    "但有一个就是我不清楚为什么IS和DR相比variance差不多\n",
    "\n",
    "尝试了好多\n",
    "\n",
    "driver 数量不一样的话...\n",
    "\n",
    "\n",
    "bias? variable?\n",
    "\n",
    "\n",
    "甚至尝试了 不用spatial\n",
    "肯定是design有了问题\n",
    "\n",
    "NN for new version? why does not work well?\n",
    "\n",
    "policy1 直接改掉吧\n",
    "\n",
    "还是depends on the setting 的\n",
    "\n",
    "弄一个大的 内部并行 会更快？inner_parallel = True. core = 36.\n",
    "\n",
    "why variance 差别这么小?\n",
    "\n",
    "好像结果也不stable\n",
    "\n",
    "QV not spatial is better?\n",
    "\n",
    "不能很像 不然会出问题？\n",
    "QV 不好？\n",
    "\n",
    "QV 自身在简化的setting 下应该很好的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下昨晚的探究\n",
    "\n",
    "看一下xinyu的内容\n",
    "\n",
    "setting不对？结果还是不对？春哥叫我做的，看一看？\n",
    "代码没有问题？N = 1的时候正常？\n",
    "想一想吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. when behav = target \n",
    "    1. validation is not good  \n",
    "    2. MSE is low\n",
    "2. close to behav\n",
    "    1. 本身就是自然会被带跑的？要比较像 比较好?\n",
    "    2. std 都不大，基本就靠bias\n",
    "    3. V / w both can not (eta learn nothing)\n",
    "    4. \n",
    "3. over attraction\n",
    "    1. do we need the correct reward leads to better performance?\n",
    "1. explore our model\n",
    "    1. under behavipour 是stationaty的吗\n",
    "    2. reward v.s. mismatch? learn something?\n",
    "1. current results\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried (03/)\n",
    "1. D_{t+1}\n",
    "2. increase noise / T \n",
    "3. u_O != u_D\n",
    "4. mean-reversion 在现在的setting下面 又不会有advantage了\n",
    "7. T_A 三个levels\n",
    "8. w_i estimation\n",
    "9. baseline effect\n",
    "2. interference effect不够强\n",
    "10. weight size\n",
    "11. decrease order differences\n",
    "2. strength of attraction: \n",
    "1. trivial\n",
    "    1. burn-in\n",
    "    1. centeralization of Q\n",
    "    1. fixed 1/2 in target [tried, not now]\n",
    "    1. number of neigh\n",
    "1. mean-reversion\n",
    "\n",
    "    \n",
    "## 03/25\n",
    "1. order size consistency. cutoff ( <1 to < 0)\n",
    "2. order 的 effect 要小一点\n",
    "3. / n_neigh\n",
    "4. standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03/27\n",
    "1. 非spatial的 我们的setting: IS/QV similar findings\n",
    "2. susan: Q similar findings\n",
    "5. check the QV / w by ourself\n",
    "\n",
    "\n",
    "2. spatial的结构 没有用进去？\n",
    "3. mean reward useless?\n",
    "4. learn nothing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting\n",
    "### Model\n",
    "0. Mis\n",
    "    1. Size: grid, N = 5 * 5, T = 48 * 14\n",
    "    2. at time t, based on $O_t$ and $D_t$ which are observations  during (t-1, t], we (randomly) choose action $A_t$ which will be implemented during (t, t + 1], and observe the reward $R_t$ which is defind as the mismatch during (t, t + 1].\n",
    "1. State:\n",
    "    1. Order: \n",
    "        1. $u^O_{l} \\sim logN(2.5, .3)$  (mean = 12.5, std = 3)\n",
    "        2. $O_{l,t} \\sim u^O_{l} + e^O_{l,t}, e^O_{l,t} \\sim (Poisson(1) - 1)$\n",
    "    2. Driver: \n",
    "        1. Attraction of $l$: $Att_{l,t} = exp(w_A * A_{l,t}) + w_O * (\\frac{O_{l,t}}{1 + D_{l,t}})$\n",
    "        2. Flow:  $D_{l, t + 1} = \\sum_{i \\in N_l} (\\frac{Att_{l,t}}{\\sum_{j \\in N_j}Att_{j,t}}D_{i,t} )$\n",
    "        3. Computation: matrix multiplication\n",
    "    4. Mismatch: $M_{t+1,g} = 0.5 * (1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|1 + D_{t+1,g}+O_{t+1,g}|}) + 0.5 * M_{t,g}$\n",
    "4. Reward: $R_{t,g} = M_{t + 1, g}min(D_{t + 1,g}, O_{t + 1,g})$ \n",
    "3. Behav policy: $A_{t,g} \\sim Bernoulli(0.5)$\n",
    "5. Target policy:  $A_{t,g} = I_{\\{u^O_l \\ge 12\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive\n",
    "2. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10) + \\vec{\\epsilon}^D_{t+1}$\n",
    "3. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10 * (2 - sin(t/48 * 2\\pi))) + \\vec{\\epsilon}^D_{t+1}$\n",
    "2. $O_{t,g} \\sim Poisson(10 * (2 - sin(t/48 * 2\\pi)))$ [no heterogeneity] -> mean is 20?\n",
    "2. $R_{i,t}=M_{i,t} * min(D_{i,t}, O_{i,t})+mean \\{M_{j,t} * min(D_{j,t}, O_{j,t})\\}_{j\\in N(i)}+\\epsilon_{i,t}$\n",
    "1. $A_{t,g} = a_g, a_g \\sim Bernoulli(0.5)$\n",
    "2. $M_{t+1,g} = 1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|D_{t+1,g}+O_{t+1,g}|}$\n",
    "2. $O_{t,g} \\sim Poisson(10)$\n",
    "1. $\\vec{D}_{t+1} = \\omega_t^{-1} \\mathbb{P}(((w_O * \\mathbf{O}_t + \\mathbf{I}) \\times (w_A * \\mathbf{A}_{t} +  \\mathbf{I}) \\times \\mathbf{Adj}  \\times (w_O * \\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + \\vec{\\epsilon}^D_{t+1})$\n",
    "            1. $w_t$ is a normalization parameter, $\\mathbf{A}_{t}$ and $\\mathbf{O}_t$ are corresponding diagonal matries, and $\\vec{\\epsilon}^D_t \\sim (Poisson(1) - 1)$\n",
    "            2. operator $\\mathbb{P}$ is defined as $\\mathbb{P}(\\vec{D}) = (max(int(D_1),1),\\dots, max(int(D_N),1))^T$\n",
    "            \n",
    "1. + \\epsilon^R_{t,g}, \\epsilon^R_{t,g} \\sim N(0,sd_R)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and competing methods\n",
    "1. Ground truth: MC-based proxy\n",
    "2. Metrics: bias, std and MSE (100 replicates)\n",
    "3. Competing methods: \n",
    "    1. \"IS\": Importance Sampling (Lihong Li) + mean-field\n",
    "    2. \"Susan\": Value funcions (Susan) + mean-field\n",
    "    3. \"DR_NS\": DR without considerinig other agents\n",
    "    3. DR without mean-field (not yet)\n",
    "8. Report results for four different $\\{a_g\\}_g$ patterns  \n",
    "\n",
    "\n",
    "1. IS:\n",
    "    1. single agent - bias\n",
    "    2. spaatial - large variance. [看draft]\n",
    "2. DR:\n",
    "    1. better than IS\n",
    "    \n",
    "### Time cost\n",
    "\n",
    "1. l = 5: 9 mins / batch on EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivations of DR2\n",
    "\n",
    "1. DR: $\\hat{Q}(\\pi(S), S)+I(A=\\pi(S))/ b(A|S) (R-\\hat{Q}(a,S))$ - (15)\n",
    "2. IS: $I(A=\\pi(S))/ b(A|S) R$\n",
    "3. Q: $\\hat{Q}(\\pi(S),S)$\n",
    "4. 2+3-1: $I(A=\\pi(S))/b(A|S) \\hat{Q}(a,S)$, it is even not doubly robust\n",
    "\n",
    "1. Q is correct: Q + IS - DR  -  true = IS - DR = wi(Q - Q)\n",
    "2. IS is correct: ... = Q - DR = -[w (R+Q - Q - V) ]\n",
    "3. ours DR: wi(R + Q - Q)\n",
    "4. ours like them: w_i*R + V - Q - w_i(R + Q' - Q)  = V - Q - w_i(Q' - Q)\n",
    "4. target: (15)\n",
    "\n",
    "any weight estimation problem?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
