{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simulation for CausalMARL\n",
    "\n",
    "## TODO\n",
    "1. 你昨天design的那个呢.感觉那个DR基本都比IS要好？感觉主要比IS好好了。主要看看可以怎么样比IS好 或者 差不多。\n",
    "5. 这个setting bias很大；ATE基本估不出来？\n",
    "1. Tuning:\n",
    "    1. Two penalties in Susan: their CV method\n",
    "    2. [w_hidden, Learning_rate] of NN: CV\n",
    "    4. Now: manually fix some values\n",
    "    3. **Q-V CV**: hard to choose neighbour actions. we can do that? but very large?   \n",
    "\n",
    "\n",
    "1. 其他参数没看出什么规律？\n",
    "    1. 加不加 sd_R 好像没有统一的趋势。还是加一个吧\n",
    "    1. sd_O 也没有看出什么趋势\n",
    "    1. sd_D 也没有看出什么趋势。。\n",
    "\n",
    "time_dependent\n",
    "n_neigh\n",
    "\n",
    "bias increased so much?\n",
    "\n",
    "\n",
    "fix a setting and discuss the trend with lambda: etc\n",
    "I can add some pattern to D and O. but the pattern needs to be shared between them.\n",
    "why no trend with T? \n",
    "怎么估计QV? sample-spliting?\n",
    "change default values\n",
    "\n",
    "policy1 直接改掉吧\n",
    "\n",
    "还是depends on the setting 的\n",
    "why variance 差别这么小?\n",
    "不能很像 不然会出问题？\n",
    "CV for NN\n",
    "\n",
    "还没有好的tuning\n",
    "\n",
    "制定计划\n",
    "\n",
    "## DONE!\n",
    "2. T=48*7=336的？就相当于一个星期，这样variance大一点\n",
    "4. setting里e_{t,g}^R可以稍微调大一点，让sd大一些\n",
    "6. 是baseline估的太差了？那theshold也不用这么大[可以10-11-12-13, 或者9-10-11-12, pi_0那个估的不行就不和pi_0比了]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sd_u_O = 0.2\n",
    "u_D = np.mean(u_O) - 2\n",
    "先试一试\n",
    "\n",
    "还是无法解决 difference 的问题\n",
    "\n",
    "为什么不solve？为什么不AB test?\n",
    "\n",
    "快速explore\n",
    "\n",
    "既要自己的估计好\n",
    "也要difference 估计的好\n",
    "还要能beat 其他方法\n",
    "\n",
    "D_t = (D_t + u_D) / 2\n",
    "\n",
    "bias 太大了, difference 太小了.\n",
    "bias 小了 和其他方法 又没有difference了\n",
    "\n",
    "只能把 不同的policy 的 difference 弄大了。\n",
    "那就把reward 给加大？\n",
    "比如reward 的 定义？\n",
    "\n",
    "\n",
    "找这种小的？\n",
    "O_threshold = 11\n",
    "MC-based mean and std of average reward:[1.1151e+01 1.0000e-02]\n",
    "   [DR/QV/IS]; [DR/QV/IS]_NO_MARL; [DR/QV/IS]_NO_MF; [DR2, V_behav]\n",
    "bias:[[0.02, 0.01, 0.02]][[0.05, 0.03, 0.04]][[11.15, 11.15, 11.15]][[0.02, 0.66]]\n",
    "std:[[0.01, 0.01, 0.0]][[0.02, 0.02, 0.03]][[0.0, 0.0, 0.0]][[0.0, 0.01]]\n",
    "MSE:[[0.02, 0.01, 0.02]][[0.05, 0.04, 0.05]][[11.15, 11.15, 11.15]][[0.02, 0.66]]\n",
    "MSE(-DR):[[0.0, -0.01, 0.0]][[0.03, 0.02, 0.03]][[11.13, 11.13, 11.13]][[0.0, 0.64]]\n",
    "***** BETTER THAN [QV, IS, DR_NO_MARL] *****\n",
    "\n",
    "\n",
    "mean-reversion 挺好的？\n",
    "\n",
    "\n",
    "这个bias 的 direction?\n",
    "我和chengchun的讨论? \n",
    "bias v.s. treatment difference 的差距不够大\n",
    "\n",
    "我感觉一个ATE 是 现在这些target之间 差距还是不够大原因。\n",
    "比如每个value 的 scale 是 九点几\n",
    "value自身的bias 是 0.3 就还好\n",
    "但是他们之间的差距也只有0.5, 0.6\n",
    "那就要bias 更小\n",
    "但又要能beat 其他的\n",
    "\n",
    "主要还是进行对比？看不出来？\n",
    "\n",
    "平均valuevalue\n",
    "\n",
    "\n",
    "scale 无意义？\n",
    "\n",
    "mean reversion + new reward 可以干到很低。\n",
    "但是difference 还是做不大\n",
    "\n",
    " u_D = np.mean(u_O) - 2\n",
    "+\n",
    "大一些的range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setting\n",
    "### Model\n",
    "0. Mis\n",
    "    1. Size: grid, N = 5 * 5, T = 48 * 14\n",
    "    2. at time t, based on $O_t$ and $D_t$ which are observations  during (t-1, t], we (randomly) choose action $A_t$ which will be implemented during (t, t + 1], and observe the reward $R_t$ which is defind as the mismatch during (t, t + 1].\n",
    "1. State:\n",
    "    1. Order: \n",
    "        1. $u^O_{l} \\sim logN(2.5, .3)$  (mean = 12.5, std = 3)\n",
    "        2. $O_{l,t} \\sim u^O_{l} + e^O_{l,t}, e^O_{l,t} \\sim (Poisson(1) - 1)$\n",
    "    2. Driver: \n",
    "        1. Attraction of $l$: $Att_{l,t} = exp(w_A * A_{l,t}) + w_O * (\\frac{O_{l,t}}{1 + D_{l,t}})$\n",
    "        2. Flow:  $D_{l, t + 1} = \\sum_{i \\in N_l} (\\frac{Att_{l,t}}{\\sum_{j \\in N_j}Att_{j,t}}D_{i,t} )$\n",
    "        3. Computation: matrix multiplication\n",
    "    4. Mismatch: $M_{t+1,g} = 0.5 * (1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|1 + D_{t+1,g}+O_{t+1,g}|}) + 0.5 * M_{t,g}$\n",
    "4. Reward: $R_{t,g} = M_{t + 1, g}min(D_{t + 1,g}, O_{t + 1,g}) + e^R_{t,g}$ \n",
    "3. Behav policy: $A_{t,g} \\sim Bernoulli(0.5)$\n",
    "5. Target policy:  $A_{t,g} = I_{\\{u^O_l \\ge 12\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive\n",
    "2. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10) + \\vec{\\epsilon}^D_{t+1}$\n",
    "3. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10 * (2 - sin(t/48 * 2\\pi))) + \\vec{\\epsilon}^D_{t+1}$\n",
    "2. $O_{t,g} \\sim Poisson(10 * (2 - sin(t/48 * 2\\pi)))$ [no heterogeneity] -> mean is 20?\n",
    "2. $R_{i,t}=M_{i,t} * min(D_{i,t}, O_{i,t})+mean \\{M_{j,t} * min(D_{j,t}, O_{j,t})\\}_{j\\in N(i)}+\\epsilon_{i,t}$\n",
    "1. $A_{t,g} = a_g, a_g \\sim Bernoulli(0.5)$\n",
    "2. $M_{t+1,g} = 1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|D_{t+1,g}+O_{t+1,g}|}$\n",
    "2. $O_{t,g} \\sim Poisson(10)$\n",
    "1. $\\vec{D}_{t+1} = \\omega_t^{-1} \\mathbb{P}(((w_O * \\mathbf{O}_t + \\mathbf{I}) \\times (w_A * \\mathbf{A}_{t} +  \\mathbf{I}) \\times \\mathbf{Adj}  \\times (w_O * \\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + \\vec{\\epsilon}^D_{t+1})$\n",
    "            1. $w_t$ is a normalization parameter, $\\mathbf{A}_{t}$ and $\\mathbf{O}_t$ are corresponding diagonal matries, and $\\vec{\\epsilon}^D_t \\sim (Poisson(1) - 1)$\n",
    "            2. operator $\\mathbb{P}$ is defined as $\\mathbb{P}(\\vec{D}) = (max(int(D_1),1),\\dots, max(int(D_N),1))^T$\n",
    "            \n",
    "1. + \\epsilon^R_{t,g}, \\epsilon^R_{t,g} \\sim N(0,sd_R)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and competing methods\n",
    "1. Ground truth: MC-based proxy\n",
    "2. Metrics: bias, std and MSE (100 replicates)\n",
    "3. Competing methods: \n",
    "    1. \"IS\": Importance Sampling (Lihong Li) + mean-field\n",
    "    2. \"Susan\": Value funcions (Susan) + mean-field\n",
    "    3. \"DR_NS\": DR without considerinig other agents\n",
    "    2. DR w/o mean-field: $R_it$ with neigh states/actions (multi-dim actions)\n",
    "8. Report results for one $\\{u_{o,g}\\}_g$ pattern with four potential target policies.\n",
    "\n",
    "1. IS:\n",
    "    1. single agent - bias\n",
    "    2. spaatial - large variance. [看draft]\n",
    "2. DR:\n",
    "    1. better than IS\n",
    "    \n",
    "### Time cost\n",
    "\n",
    "1. l = 5: 11mins for 2 reps and one setting/pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
