{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simulation for CausalMARL\n",
    "\n",
    "## TODO\n",
    "1. Tuning:\n",
    "    1. Two penalties in Susan: their CV method\n",
    "    2. [w_hidden, Learning_rate] of NN: CV\n",
    "    4. Now: manually fix some values\n",
    "    3. **Q-V CV**: hard to choose neighbour actions. we can do that? but very large?   \n",
    "\n",
    "\n",
    "## DONE!\n",
    "2. T=48*7=336的？就相当于一个星期，这样variance大一点\n",
    "4. setting里e_{t,g}^R可以稍微调大一点，让sd大一些\n",
    "6. 是baseline估的太差了？那theshold也不用这么大[可以10-11-12-13, 或者9-10-11-12, pi_0那个估的不行就不和pi_0比了]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. chengchun\n",
    "2. US prediction\n",
    "3. SIR\n",
    "\n",
    "CV\n",
    "代码给那谁\n",
    "还有哪儿可以改呢？\n",
    "\n",
    "再看看T的trend?\n",
    "\n",
    "policy search 可以加速？\n",
    "trend？想不清楚？\n",
    "该怎么做这个evaluation 怎么讲这个story呢？\n",
    "\n",
    "画图的话加上non-spatial的话 会高太多？\n",
    "\n",
    "\n",
    "代码没出问题吧？\n",
    "simple 会怎样？\n",
    "\n",
    "less bias\n",
    "large variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "还是depends on the setting 的\n",
    "\n",
    "why variance 差别这么小?\n",
    "\n",
    "不能很像 不然会出问题？\n",
    "\n",
    "要比non-spatial好很多？- how?\n",
    "why bias increase? \n",
    "tuning 吗\n",
    "\n",
    "1. NO_MF later\n",
    "3. mean_reversion?\n",
    "4. u_O_u_D\n",
    "记录下那个setting即可\n",
    "\n",
    "整理下code 给他就好了\n",
    "好 那我就跑一下 然后发给他们 然后开始写一写\n",
    "\n",
    "主要是比no_MARL的要好，然后majority of sertting DR比IS好就行\n",
    "\n",
    "画图\n",
    "\n",
    "就光fixed time 比他们好都不容易？\n",
    "还要关于时间有trend?\n",
    "\n",
    "reward 的 variance 弄大 应该会挺容易能比较好地估计的。\n",
    "如何地快速弄出difference 的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "1. value estimations are all good\n",
    "2. difference estimations are all good\n",
    "3. better than IS in 1, 2\n",
    "4. better than single agent (in bias) in 1, 2\n",
    "5. better than all neigh infomation (in variance) in 1, 2\n",
    "\n",
    "\n",
    "1. ours\n",
    "2. average\n",
    "3. QV \n",
    "4. DR_no_MARL\n",
    "5. DR_no_Mean_Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "1. Noise\n",
    "    1. sd_O\n",
    "    2. sd_R\n",
    "    3. sd_D\n",
    "1. mean_reversion: D_t = (D_t + u_D) / 2\n",
    "2. spatial\n",
    "    1. w_O\n",
    "    2. w_A\n",
    "2. hyperparameters of NN and Q/V\n",
    "2. Difference\n",
    "    1. sd_u_O\n",
    "    2. u_D = np.mean(u_O) - 2\n",
    "4. reward def\n",
    "5. T\n",
    "6. l\n",
    "7. threshold_range\n",
    "8. pattern seed\n",
    "9. p_behav\n",
    "10. n_neigh (simple = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### findings\n",
    "1. mean reversion + new reward 可以干到很低。但是difference 还是做不大\n",
    "2. increase error can increase variance. but not very much\n",
    "3. mean-reversion 挺好的？\n",
    "\n",
    "3. 如果降低bias? overfitting + less penalty?\n",
    "轻松beat 其他？\n",
    "和chengchun的讨论？\n",
    "\n",
    "大一些的range\n",
    "平均value?\n",
    "\n",
    "试一试 \n",
    "\n",
    "只能把 不同的policy 的 difference 弄大了。\n",
    "那就把reward 给加大？比如定义？\n",
    "\n",
    "给奖励的好处？ difference 加大会咋样？\n",
    "\n",
    "可能可以四个pattern 然后 x 轴是threshold?\n",
    "\n",
    "那这个trend 没有意义了吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Setting\n",
    "### Model\n",
    "0. Mis\n",
    "    1. Size: grid, N = 5 * 5, T = 48 * 14\n",
    "    2. at time t, based on $O_t$ and $D_t$ which are observations  during (t-1, t], we (randomly) choose action $A_t$ which will be implemented during (t, t + 1], and observe the reward $R_t$ which is defind as the mismatch during (t, t + 1].\n",
    "1. State:\n",
    "    1. Order: \n",
    "        1. $u^O_{l} \\sim logN(2.5, .3)$  (mean = 12.5, std = 3)\n",
    "        2. $O_{l,t} \\sim u^O_{l} + e^O_{l,t}, e^O_{l,t} \\sim (Poisson(1) - 1)$\n",
    "    2. Driver: \n",
    "        1. Attraction of $l$: $Att_{l,t} = exp(w_A * A_{l,t}) + w_O * (\\frac{O_{l,t}}{1 + D_{l,t}})$\n",
    "        2. Flow:  $D_{l, t + 1} = \\sum_{i \\in N_l} (\\frac{Att_{l,t}}{\\sum_{j \\in N_j}Att_{j,t}}D_{i,t} )$\n",
    "        3. Computation: matrix multiplication\n",
    "    4. Mismatch: $M_{t+1,g} = 0.5 * (1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|1 + D_{t+1,g}+O_{t+1,g}|}) + 0.5 * M_{t,g}$\n",
    "4. Reward: $R_{t,g} = M_{t + 1, g}min(D_{t + 1,g}, O_{t + 1,g}) + e^R_{t,g}$ \n",
    "3. Behav policy: $A_{t,g} \\sim Bernoulli(0.5)$\n",
    "5. Target policy:  $A_{t,g} = I_{\\{u^O_l \\ge 12\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive\n",
    "2. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10) + \\vec{\\epsilon}^D_{t+1}$\n",
    "3. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10 * (2 - sin(t/48 * 2\\pi))) + \\vec{\\epsilon}^D_{t+1}$\n",
    "2. $O_{t,g} \\sim Poisson(10 * (2 - sin(t/48 * 2\\pi)))$ [no heterogeneity] -> mean is 20?\n",
    "2. $R_{i,t}=M_{i,t} * min(D_{i,t}, O_{i,t})+mean \\{M_{j,t} * min(D_{j,t}, O_{j,t})\\}_{j\\in N(i)}+\\epsilon_{i,t}$\n",
    "1. $A_{t,g} = a_g, a_g \\sim Bernoulli(0.5)$\n",
    "2. $M_{t+1,g} = 1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|D_{t+1,g}+O_{t+1,g}|}$\n",
    "2. $O_{t,g} \\sim Poisson(10)$\n",
    "1. $\\vec{D}_{t+1} = \\omega_t^{-1} \\mathbb{P}(((w_O * \\mathbf{O}_t + \\mathbf{I}) \\times (w_A * \\mathbf{A}_{t} +  \\mathbf{I}) \\times \\mathbf{Adj}  \\times (w_O * \\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + \\vec{\\epsilon}^D_{t+1})$\n",
    "    1. $w_t$ is a normalization parameter, $\\mathbf{A}_{t}$ and $\\mathbf{O}_t$ are corresponding diagonal matries, and $\\vec{\\epsilon}^D_t \\sim (Poisson(1) - 1)$\n",
    "    2. operator $\\mathbb{P}$ is defined as $\\mathbb{P}(\\vec{D}) = (max(int(D_1),1),\\dots, max(int(D_N),1))^T$      \n",
    "1. + \\epsilon^R_{t,g}, \\epsilon^R_{t,g} \\sim N(0,sd_R)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and competing methods\n",
    "1. Ground truth: MC-based proxy\n",
    "2. Metrics: bias, std and MSE (100 replicates)\n",
    "3. Competing methods: \n",
    "    1. \"IS\": Importance Sampling (Lihong Li) + mean-field\n",
    "    2. \"Susan\": Value funcions (Susan) + mean-field\n",
    "    3. \"DR_NS\": DR without considerinig other agents\n",
    "    2. DR w/o mean-field: $R_it$ with neigh states/actions (multi-dim actions)\n",
    "8. Report results for one $\\{u_{o,g}\\}_g$ pattern with four potential target policies.\n",
    "\n",
    "1. IS:\n",
    "    1. single agent - bias\n",
    "    2. spaatial - large variance. [看draft]\n",
    "2. DR:\n",
    "    1. better than IS\n",
    "    \n",
    "### Time cost\n",
    "\n",
    "1. l = 5: 11mins for 2 reps and one setting/pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
