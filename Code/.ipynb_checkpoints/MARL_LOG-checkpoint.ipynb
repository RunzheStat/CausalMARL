{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simulation for CausalMARL\n",
    "\n",
    "## TODO\n",
    "1. 你昨天design的那个呢.感觉那个DR基本都比IS要好？感觉主要比IS好好了。主要看看可以怎么样比IS好 或者 差不多。\n",
    "5. 这个setting bias很大；ATE基本估不出来？\n",
    "1. Tuning:\n",
    "    1. Two penalties in Susan: their CV method\n",
    "    2. [w_hidden, Learning_rate] of NN: CV\n",
    "    4. Now: manually fix some values\n",
    "    3. **Q-V CV**: hard to choose neighbour actions. we can do that? but very large?   \n",
    "\n",
    "\n",
    "1. 其他参数没看出什么规律？\n",
    "    1. 加不加 sd_R 好像没有统一的趋势。还是加一个吧\n",
    "    1. sd_O 也没有看出什么趋势\n",
    "    1. sd_D 也没有看出什么趋势。。\n",
    "\n",
    "time_dependent\n",
    "n_neigh\n",
    "\n",
    "bias increased so much?\n",
    "\n",
    "\n",
    "fix a setting and discuss the trend with lambda: etc\n",
    "I can add some pattern to D and O. but the pattern needs to be shared between them.\n",
    "why no trend with T? \n",
    "怎么估计QV? sample-spliting?\n",
    "change default values\n",
    "\n",
    "policy1 直接改掉吧\n",
    "\n",
    "还是depends on the setting 的\n",
    "why variance 差别这么小?\n",
    "不能很像 不然会出问题？\n",
    "CV for NN\n",
    "\n",
    "还没有好的tuning\n",
    "\n",
    "## DONE!\n",
    "2. T=48*7=336的？就相当于一个星期，这样variance大一点\n",
    "4. setting里e_{t,g}^R可以稍微调大一点，让sd大一些\n",
    "6. 是baseline估的太差了？那theshold也不用这么大[可以10-11-12-13, 或者9-10-11-12, pi_0那个估的不行就不和pi_0比了]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. O follows poisson (larger variance?)\n",
    "2. larger noise\n",
    "sd_D = 10 #1\n",
    "sd_R = 10 #1\n",
    "\n",
    "要比non-spatial好很多？\n",
    "M_in_R is True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thre_range = [80, 85, 90, 95, 100, 105, 110]\n",
    "location 太多了？\n",
    "看看规律吧\n",
    "\n",
    "smaller bias. how?\n",
    "\n",
    "present trend with sample size? 试一试？\n",
    "\n",
    "找出最优的 (how?) 然后线性 组合？\n",
    "\n",
    "n_neigh 慢慢琢磨? \n",
    "\n",
    "simple = True + small T + smaller sd\n",
    "focus on pattern 0\n",
    "\n",
    "同时改了还几个\n",
    "如何把difference 弄大一些？\n",
    "\n",
    "T: no trend\n",
    "MSE\n",
    "bias\n",
    "spatial\n",
    "consistency\n",
    "\n",
    "target 换成其他几个random的 fixed 的? \n",
    "\n",
    "or just randomly  generate the target policy\n",
    "要实际一些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "1. value estimations are all good\n",
    "2. difference estimations are all good\n",
    "3. better than IS in 1, 2\n",
    "4. better than single agent (in bias) in 1, 2\n",
    "5. better than all neigh infomation (in variance) in 1, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "1. Noise\n",
    "    1. sd_O\n",
    "    2. sd_R\n",
    "    3. sd_D\n",
    "1. D_t = (D_t + u_D) / 2\n",
    "2. spatial\n",
    "    1. w_O\n",
    "    2. w_A\n",
    "2. hyperparameters of NN and Q/V\n",
    "2. Difference\n",
    "    1. sd_u_O\n",
    "    2. u_D = np.mean(u_O) - 2\n",
    "3. scale?   \n",
    "4. reward def\n",
    "5. T\n",
    "6. l\n",
    "7. thre_range\n",
    "8. pattern seed\n",
    "9. p_behav\n",
    "10. n_neigh (simple = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### findings\n",
    "1. mean reversion + new reward 可以干到很低。但是difference 还是做不大\n",
    "2. increase error can increase variance. but not very much\n",
    "3. mean-reversion 挺好的？\n",
    "4. bias 的 叠加方向: 主要是bias 和 difference 在同一个scale上面。但是difference大了, bias 应该也会increase吧\n",
    "\n",
    "### 尝试了的\n",
    "1. 试试大的error\n",
    "2. 和 l [reward 是新的了]\n",
    "3. 如果降低bias? overfitting + less penalty?\n",
    "\n",
    "大一些的range\n",
    "平均value?\n",
    "快速explore\n",
    "\n",
    "\n",
    "\n",
    "什么情况下才能看出来？\n",
    "\n",
    "\n",
    "只能把 不同的policy 的 difference 弄大了。\n",
    "那就把reward 给加大？比如定义？\n",
    "\n",
    "\n",
    "到底目的是什么？\n",
    "如何重现好结果？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.     u_D = np.mean(u_O) - u_D_u_O\n",
    "2. 取消了 mean-reversion\n",
    "\n",
    "3. 不用找之前的好的setting了\n",
    "\n",
    "核心就是要找一个basis 比其他两个都好很多 \n",
    "\n",
    "then it is enough?\n",
    "\n",
    "但也不是完美的吧\n",
    "\n",
    "给奖励的好处？ difference 加大会咋样？\n",
    "\n",
    "找一个能估计的好的base line?\n",
    "但是bias的方向也很重要?\n",
    "所以还是很玄妙吧\n",
    "\n",
    "u_O_u_D\n",
    "为何会好？\n",
    "\n",
    "\n",
    "自动找一个好的setting\n",
    "为什么我的结果不再一样了？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setting\n",
    "### Model\n",
    "0. Mis\n",
    "    1. Size: grid, N = 5 * 5, T = 48 * 14\n",
    "    2. at time t, based on $O_t$ and $D_t$ which are observations  during (t-1, t], we (randomly) choose action $A_t$ which will be implemented during (t, t + 1], and observe the reward $R_t$ which is defind as the mismatch during (t, t + 1].\n",
    "1. State:\n",
    "    1. Order: \n",
    "        1. $u^O_{l} \\sim logN(2.5, .3)$  (mean = 12.5, std = 3)\n",
    "        2. $O_{l,t} \\sim u^O_{l} + e^O_{l,t}, e^O_{l,t} \\sim (Poisson(1) - 1)$\n",
    "    2. Driver: \n",
    "        1. Attraction of $l$: $Att_{l,t} = exp(w_A * A_{l,t}) + w_O * (\\frac{O_{l,t}}{1 + D_{l,t}})$\n",
    "        2. Flow:  $D_{l, t + 1} = \\sum_{i \\in N_l} (\\frac{Att_{l,t}}{\\sum_{j \\in N_j}Att_{j,t}}D_{i,t} )$\n",
    "        3. Computation: matrix multiplication\n",
    "    4. Mismatch: $M_{t+1,g} = 0.5 * (1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|1 + D_{t+1,g}+O_{t+1,g}|}) + 0.5 * M_{t,g}$\n",
    "4. Reward: $R_{t,g} = M_{t + 1, g}min(D_{t + 1,g}, O_{t + 1,g}) + e^R_{t,g}$ \n",
    "3. Behav policy: $A_{t,g} \\sim Bernoulli(0.5)$\n",
    "5. Target policy:  $A_{t,g} = I_{\\{u^O_l \\ge 12\\}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive\n",
    "2. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10) + \\vec{\\epsilon}^D_{t+1}$\n",
    "3. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10 * (2 - sin(t/48 * 2\\pi))) + \\vec{\\epsilon}^D_{t+1}$\n",
    "2. $O_{t,g} \\sim Poisson(10 * (2 - sin(t/48 * 2\\pi)))$ [no heterogeneity] -> mean is 20?\n",
    "2. $R_{i,t}=M_{i,t} * min(D_{i,t}, O_{i,t})+mean \\{M_{j,t} * min(D_{j,t}, O_{j,t})\\}_{j\\in N(i)}+\\epsilon_{i,t}$\n",
    "1. $A_{t,g} = a_g, a_g \\sim Bernoulli(0.5)$\n",
    "2. $M_{t+1,g} = 1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|D_{t+1,g}+O_{t+1,g}|}$\n",
    "2. $O_{t,g} \\sim Poisson(10)$\n",
    "1. $\\vec{D}_{t+1} = \\omega_t^{-1} \\mathbb{P}(((w_O * \\mathbf{O}_t + \\mathbf{I}) \\times (w_A * \\mathbf{A}_{t} +  \\mathbf{I}) \\times \\mathbf{Adj}  \\times (w_O * \\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + \\vec{\\epsilon}^D_{t+1})$\n",
    "            1. $w_t$ is a normalization parameter, $\\mathbf{A}_{t}$ and $\\mathbf{O}_t$ are corresponding diagonal matries, and $\\vec{\\epsilon}^D_t \\sim (Poisson(1) - 1)$\n",
    "            2. operator $\\mathbb{P}$ is defined as $\\mathbb{P}(\\vec{D}) = (max(int(D_1),1),\\dots, max(int(D_N),1))^T$\n",
    "            \n",
    "1. + \\epsilon^R_{t,g}, \\epsilon^R_{t,g} \\sim N(0,sd_R)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and competing methods\n",
    "1. Ground truth: MC-based proxy\n",
    "2. Metrics: bias, std and MSE (100 replicates)\n",
    "3. Competing methods: \n",
    "    1. \"IS\": Importance Sampling (Lihong Li) + mean-field\n",
    "    2. \"Susan\": Value funcions (Susan) + mean-field\n",
    "    3. \"DR_NS\": DR without considerinig other agents\n",
    "    2. DR w/o mean-field: $R_it$ with neigh states/actions (multi-dim actions)\n",
    "8. Report results for one $\\{u_{o,g}\\}_g$ pattern with four potential target policies.\n",
    "\n",
    "1. IS:\n",
    "    1. single agent - bias\n",
    "    2. spaatial - large variance. [看draft]\n",
    "2. DR:\n",
    "    1. better than IS\n",
    "    \n",
    "### Time cost\n",
    "\n",
    "1. l = 5: 11mins for 2 reps and one setting/pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
