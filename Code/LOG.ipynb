{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simulation for CausalMARL\n",
    "\n",
    "## TODO\n",
    "1. debug: \n",
    "    1. try DGP with no neigh [identity ADJ]\n",
    "1. Tuning:\n",
    "    1. Two penalties in Susan: their CV method\n",
    "    2. [w_hidden, Learning_rate] of NN: CV\n",
    "    4. Now: manually fix some values\n",
    "    3. **Q-V CV**: hard to choose neighbour actions. we can do that? but very large?\n",
    "5. spatial information - model\n",
    "2. DR w/o mean-field \n",
    "    1. R_it: \n",
    "        1. with neigh states/actions as appending\n",
    "        2. with global states/actions as appending \n",
    "    2. \\bar{R}_t + global states and actions\n",
    "    3. multi-dim actions    \n",
    "\n",
    "1. why wi estimation is not good?\n",
    "    1. why R2 close to  0? what is our attempt?\n",
    "    2. density close to zero, den ratio is very large, even after normalization, std is large\n",
    "1. how to evaluate the true density and Q/V functions?\n",
    "2. Check the current model and policy\n",
    "    1. stationary?\n",
    "1. Design a toy example\n",
    "    1. need DR (how?)\n",
    "        1. weight learned with a linear function\n",
    "        2. Q learned wtih a linear function\n",
    "    2. need spatial (mean state and action)\n",
    "        1. R_i = X1 * X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. when behav = target \n",
    "    1. validation is not good  \n",
    "    2. MSE is low\n",
    "2. close to behav\n",
    "    1. 本身就是自然会被带跑的？要比较像 比较好?\n",
    "    2. std 都不大，基本就靠bias\n",
    "    3. V / w both can not (eta learn nothing)\n",
    "    4. \n",
    "3. over attraction\n",
    "    1. do we need the correct reward leads to better performance?\n",
    "1. explore our model\n",
    "    1. under behavipour 是stationaty的吗\n",
    "    2. reward v.s. mismatch? learn something?\n",
    "1. current results\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried (03/)\n",
    "1. D_{t+1}\n",
    "2. increase noise / T \n",
    "3. u_O != u_D\n",
    "4. mean-reversion 在现在的setting下面 又不会有advantage了\n",
    "7. T_A 三个levels\n",
    "8. w_i estimation\n",
    "9. baseline effect\n",
    "2. interference effect不够强\n",
    "10. 可以看看那些weight都多大吗\n",
    "11. decrease order differences\n",
    "2. strength of attraction: \n",
    "1. trivial\n",
    "    1. burn-in\n",
    "    1. centeralization of Q\n",
    "    1. fixed 1/2 in target [tried, not now]\n",
    "    1. number of neigh\n",
    "1. mean-reversion\n",
    "\n",
    "    \n",
    "## 03/25\n",
    "1. order size consistency. cutoff ( <1 to < 0)\n",
    "2. order 的 effect 要小一点\n",
    "3. / n_neigh\n",
    "4. standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03/27\n",
    "1. 我直接试一试 非spatial的吧\n",
    "5. check the QV / w by ourself\n",
    "\n",
    "\n",
    "2. spatial的结构 没有用进去？\n",
    "3. mean reward useless?\n",
    "4. learn nothing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting\n",
    "### Model\n",
    "0. Mis\n",
    "    1. Size: grid, N = 5 * 5, T = 48 * 14\n",
    "    2. at time t, based on $O_t$ and $D_t$ which are observations  during (t-1, t], we (randomly) choose action $A_t$ which will be implemented during (t, t + 1], and observe the reward $R_t$ which is defind as the mismatch during (t, t + 1].\n",
    "1. State:\n",
    "    1. Order: \n",
    "        1. $O_{t,g} \\sim Poisson(10)$\n",
    "        2. $O_{t,g} \\sim Poisson(10 * (2 - sin(t/48 * 2\\pi)))$ [no heterogeneity] -> mean is 20?\n",
    "    2. Driver: \n",
    "        1. Current: $\\vec{D}_{t+1} = \\omega_t^{-1} \\mathbb{P}(((w_O * \\mathbf{O}_t + \\mathbf{I}) \\times (w_A * \\mathbf{A}_{t} +  \\mathbf{I}) \\times \\mathbf{Adj}  \\times (w_O * \\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + \\vec{\\epsilon}^D_{t+1})$\n",
    "            1. $w_t$ is a normalization parameter, $\\mathbf{A}_{t}$ and $\\mathbf{O}_t$ are corresponding diagonal matries, and $\\vec{\\epsilon}^D_t \\sim (Poisson(1) - 1)$\n",
    "            2. operator $\\mathbb{P}$ is defined as $\\mathbb{P}(\\vec{D}) = (max(int(D_1),1),\\dots, max(int(D_N),1))^T$\n",
    "        2. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10) + \\vec{\\epsilon}^D_{t+1}$\n",
    "        3. $\\vec{D}_{t+1} = \\frac{1}{2}(\\omega_t^{-1} (\\mathbf{O}_t \\times (\\mathbf{A}_{t} + \\mathbf{I}) \\times \\mathbf{Adj}  \\times (\\mathbf{O}_t + \\mathbf{I})^{-1})  \\vec{D}_{t} + 10 * (2 - sin(t/48 * 2\\pi))) + \\vec{\\epsilon}^D_{t+1}$\n",
    "    4. Mismatch: \n",
    "        1. $M_{t+1,g} = 0.5 * (1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|D_{t+1,g}+O_{t+1,g}|}) + 0.5 * M_{t,g}$\n",
    "        2. $M_{t+1,g} = 1-\\frac{|D_{t+1,g}-O_{t+1,g}|}{|D_{t+1,g}+O_{t+1,g}|}$\n",
    "3. Action: $A_{t,g} \\sim Bernoulli(0.5)$\n",
    "4. Reward: \n",
    "    1. current: $R_{t,g} = M_{t + 1, g}min(D_{t + 1,g}, O_{t + 1,g}) + \\epsilon^R_{t,g}, \\epsilon^R_{t,g} \\sim N(0,sd_R)$\n",
    "    2. $R_{i,t}=M_{i,t} * min(D_{i,t}, O_{i,t})+mean \\{M_{j,t} * min(D_{j,t}, O_{j,t})\\}_{j\\in N(i)}+\\epsilon_{i,t}$\n",
    "5. Target policy: \n",
    "    1. $A_{t,g} = a_g, a_g \\sim Bernoulli(0.5)$\n",
    "    2. current: $A_{t,g} = I_{\\{u_{O,g} \\ge 12\\}}$\n",
    "6. Ts and Ta is the average function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and competing methods\n",
    "1. Ground truth: MC-based proxy\n",
    "2. Metrics: bias, std and MSE (100 replicates)\n",
    "3. Competing methods: \n",
    "    1. \"IS\": Importance Sampling (Lihong Li) + mean-field\n",
    "    2. \"Susan\": Value funcions (Susan) + mean-field\n",
    "    3. \"DR_NS\": DR without considerinig other agents\n",
    "    3. DR without mean-field (not yet)\n",
    "8. Report results for four different $\\{a_g\\}_g$ patterns  \n",
    "\n",
    "\n",
    "1. IS:\n",
    "    1. single agent - bias\n",
    "    2. spaatial - large variance. [看draft]\n",
    "2. DR:\n",
    "    1. better than IS\n",
    "    \n",
    "### Time cost\n",
    "\n",
    "1. l = 5: 9 mins / batch on EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivations of DR2\n",
    "\n",
    "1. DR: $\\hat{Q}(\\pi(S), S)+I(A=\\pi(S))/ b(A|S) (R-\\hat{Q}(a,S))$ - (15)\n",
    "2. IS: $I(A=\\pi(S))/ b(A|S) R$\n",
    "3. Q: $\\hat{Q}(\\pi(S),S)$\n",
    "4. 2+3-1: $I(A=\\pi(S))/b(A|S) \\hat{Q}(a,S)$, it is even not doubly robust\n",
    "\n",
    "1. Q is correct: Q + IS - DR  -  true = IS - DR = wi(Q - Q)\n",
    "2. IS is correct: ... = Q - DR = -[w (R+Q - Q - V) ]\n",
    "3. ours DR: wi(R + Q - Q)\n",
    "4. ours like them: w_i*R + V - Q - w_i(R + Q' - Q)  = V - Q - w_i(Q' - Q)\n",
    "4. target: (15)\n",
    "\n",
    "any weight estimation problem?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
