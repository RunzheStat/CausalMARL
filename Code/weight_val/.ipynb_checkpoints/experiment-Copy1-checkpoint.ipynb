{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(u_O) =  157.3 mean(u_O) =  93.7\n",
      "O_threshold = 100\n",
      "means of Order: \n",
      "\n",
      "89.6 98.6 46.6 141.0 55.2 \n",
      "\n",
      "79.0 112.6 68.9 73.6 77.3 \n",
      "\n",
      "113.8 157.3 101.0 72.1 113.5 \n",
      "\n",
      "85.1 99.5 129.4 81.3 100.2 \n",
      "\n",
      "78.0 96.1 106.4 75.3 91.5 \n",
      "\n",
      "target policy: \n",
      "\n",
      "0 0 0 1 0 \n",
      "\n",
      "0 1 0 0 0 \n",
      "\n",
      "1 1 1 0 1 \n",
      "\n",
      "0 0 1 0 1 \n",
      "\n",
      "0 0 1 0 0 \n",
      "\n",
      "number of reward locations:  9\n",
      "O_threshold = 101\n",
      "number of reward locations:  8\n",
      "O_threshold = 105\n",
      "number of reward locations:  7\n",
      "O_threshold = 110\n",
      "number of reward locations:  6\n",
      "here 200\n"
     ]
    }
   ],
   "source": [
    "# export openblas_num_threads=1; export OMP_NUM_THREADS=1; python simu.py\n",
    "\n",
    "from utils import * \n",
    "from weight import * \n",
    "from simu_funs import *  \n",
    "from main_200 import *\n",
    "a = now()\n",
    "\n",
    "rep_times = 96\n",
    "region_parallel = False\n",
    "full_parallel = False\n",
    "pattern_seed = 2\n",
    "sd_u_O = 25\n",
    "w_O = .5\n",
    "w_A = 1.5\n",
    "u_D = 80\n",
    "thre_range = [100, 101, 105, 110]\n",
    "\n",
    "penalty_range = [[3e-4, 1e-4, 5e-5], [3e-4, 1e-4, 5e-5]]\n",
    "with_NO_MARL = True\n",
    "with_IS = True\n",
    "with_MF = True\n",
    "n_layer = 3\n",
    "max_iteration = 5001\n",
    "Learning_rate = 5e-4\n",
    "w_hidden = 30\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "aim = \"final_sd\"\n",
    "\n",
    "sd_R = 0\n",
    "day = 7\n",
    "results, res_real = [], []\n",
    "\n",
    "\n",
    "l = 5\n",
    "\n",
    "npseed(pattern_seed)\n",
    "u_O = rnorm(100, sd_u_O, l**2) \n",
    "\n",
    "\n",
    "print(\"max(u_O) = \", np.round(max(u_O), 1), \"mean(u_O) = \", np.round(np.mean(u_O), 1))\n",
    "\n",
    "# generate the corresponding target plicy\n",
    "target_policys = []\n",
    "n_tp = len(thre_range)\n",
    "for i in range(n_tp):\n",
    "    O_thre = thre_range[i]\n",
    "    print(\"O_threshold = \" + str(thre_range[i]))\n",
    "    if i == 0: \n",
    "        target_policy = simu_target_policy_pattern(l = l, u_O = u_O, threshold =  O_thre, print_flag = \"all\") # \"all\"\n",
    "    else:\n",
    "        target_policy = simu_target_policy_pattern(l = l, u_O = u_O, threshold =  O_thre, print_flag = \"None\") # \"policy_only\"\n",
    "    target_policys.append(target_policy)\n",
    "\n",
    "# generate the adj for the grid\n",
    "neigh = adj2neigh(getAdjGrid(l))\n",
    "\n",
    "\n",
    "seed = 1 \n",
    "l = 5\n",
    "T = day * 48\n",
    "t_func = None\n",
    "sd_D = None\n",
    "sd_O = None\n",
    "inner_parallel = False\n",
    "CV_QV = True\n",
    "penalty_NMF = [[1e-3], [1e-3]]\n",
    "dim_S_plus_Ts = 3 + 3\n",
    "epsilon = 1e-6\n",
    "\n",
    "##########################################################################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Output: a len-n-target of len-est results\n",
    "\"\"\"\n",
    "npseed(seed)\n",
    "N = l ** 2\n",
    "def behav(s, a):\n",
    "    return 0.5\n",
    "behav = list(itertools.repeat(behav, N))\n",
    "\n",
    "def Ts(S_neigh):\n",
    "    return np.mean(S_neigh, 0)\n",
    "def Ta(A_neigh):\n",
    "    return Ta_disc(np.mean(A_neigh, 0))\n",
    "\n",
    "# observed data following behav\n",
    "data, adj_mat, details = DG_once(seed = seed, l = l, T = T, u_D = u_D, \n",
    "                                 u_O = u_O, \n",
    "                                 t_func = t_func,  \n",
    "                                 sd_D = sd_D, sd_R = sd_R, sd_O = sd_O, \n",
    "                                 w_A = w_A, w_O = w_O)\n",
    "\n",
    "target_policy = target_policys[0]\n",
    "\n",
    "val_paras = [sd_D, sd_R, u_D, sd_O, w_A, w_O, u_O]\n",
    "\n",
    "# OPE\n",
    "a = now()\n",
    "value_targets = []\n",
    "count = 0\n",
    "n_target = len(target_policys)\n",
    "value_estimators = V_DR(data = data, tp = target_policy, bp = behav, \n",
    "                        adj_mat = adj_mat, dim_S_plus_Ts = dim_S_plus_Ts, \n",
    "                     t_func = t_func, \n",
    "                        is_weight_val = True, val_paras = val_paras, \n",
    "                     Ts = Ts, Ta = Ta, penalty = penalty_range, penalty_NMF = penalty_NMF, \n",
    "                        n_layer = n_layer, CV_QV = CV_QV, \n",
    "                    w_hidden = w_hidden, lr = Learning_rate,  \n",
    "                    batch_size = batch_size, max_iteration = max_iteration, epsilon = epsilon, \n",
    "                        with_MF = with_MF, with_NO_MARL = with_NO_MARL, with_IS = with_IS, \n",
    "                     inner_parallel = inner_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utility import * \n",
    "from weight import * \n",
    "from simu_funs import *  \n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample input for specifying t_func and tp:\n",
    "\n",
    "### 1. tp depending on order and driver\n",
    "\n",
    "# assume s = [o, d, mismatch]r\n",
    "pi_adaptive = []\n",
    "threshold = 1\n",
    "for i in range(9):\n",
    "    def tp_i(s, a = 0, random_choose = False):\n",
    "        is_reward = int(s[0] / s[1] > threshold) # when order > driver, we give subsidy\n",
    "        if random_choose:\n",
    "            return is_reward\n",
    "        else:\n",
    "            return int(a == is_reward)\n",
    "    pi_adaptive.append(tp_i)\n",
    "target_policy = pi_adaptive\n",
    "\n",
    "t_func = None\n",
    "\n",
    "### 2. tp depending on location and time_of_day - option 1 \n",
    "# (RECOMMENDED) it may reduce variance with such a discretized time stamp\n",
    "\n",
    "# if you need a more complex dependecy on time and location, please modify this exmaple or let me know\n",
    "\n",
    "def t_func(t):\n",
    "    # the time state variable is an indicator for rush hours\n",
    "    return int(t % 48 in [36,37,38,39,40])\n",
    "range_transformed_time_index = [0, 1] # can be extended to more than two time slices\n",
    "\n",
    "location_give_subsidy = [0, 1, 2] # other regions will never receive subsidy\n",
    "\n",
    "# adaptive_tp_dictionary is a len-N of len-N_range_transformed_time_index of binary value indicating \n",
    "# whether or not this space-time slice should receive subsidy\n",
    "# For example: \n",
    "\n",
    "adaptive_action_list = []\n",
    "adaptive_tp_dictionary = []\n",
    "for region_index in range(9):\n",
    "    region_action=[]\n",
    "    for time_index in range_transformed_time_index:\n",
    "        if(time_index == 1 and region_index in location_give_subsidy):\n",
    "            adaptive_action_list.append(1)\n",
    "            region_action.append(1)\n",
    "        else:\n",
    "            adaptive_action_list.append(0)\n",
    "            region_action.append(0)\n",
    "    adaptive_tp_dictionary.append(region_action)\n",
    "print(adaptive_tp_dictionary)\n",
    "\n",
    "\n",
    "pi_adaptive = []\n",
    "for i in range(9):\n",
    "    def tp_i(s, a = 0, random_choose = False, fixed_policy_i = adaptive_tp_dictionary[i]): # , fixed_policy_i = adaptive_tp_dictionary[i]\n",
    "        t = int(s[3]) # already transformed by t_func\n",
    "        if random_choose:\n",
    "            return fixed_policy_i[t]\n",
    "        else:\n",
    "            return int(a == fixed_policy_i[t])\n",
    "    pi_adaptive.append(tp_i)\n",
    "target_policy = pi_adaptive\n",
    "\n",
    "\n",
    "### 3. tp depending on location and time_of_day - option 2\n",
    "def t_func(t):\n",
    "    return int(t % 48)\n",
    "\n",
    "adaptive_action_list = []\n",
    "adaptive_tp_dictionary = []\n",
    "for region_index in range(9):\n",
    "    region_action=[]\n",
    "    for time_index in range(336):\n",
    "        if(time_index%48 in [36,37,38,39,40]):\n",
    "            adaptive_action_list.append(1)\n",
    "            region_action.append(1)\n",
    "        else:\n",
    "            adaptive_action_list.append(0)\n",
    "            region_action.append(0)\n",
    "    adaptive_tp_dictionary.append(region_action)\n",
    "# print(adaptive_tp_dictionary)\n",
    "\n",
    "pi_adaptive = []\n",
    "for i in range(9):\n",
    "    def tp_i(s, a = 0, random_choose = False, fixed_policy_i = adaptive_tp_dictionary[i]):\n",
    "        t = int(s[3])\n",
    "        if random_choose:\n",
    "            return fixed_policy_i[t]\n",
    "        else:\n",
    "            return int(a == fixed_policy_i[t])\n",
    "    pi_adaptive.append(tp_i)\n",
    "target_policy = pi_adaptive\n",
    "\n",
    "\n",
    "### 4. tp depending on location, time_of_day and day_of_week\n",
    "\n",
    "day_of_week_need_subsidy = [0,1,2,3,4] # for example, only subsidy for rush hours in weekdays\n",
    "time_index_range_in_a_week_need_subsidy = []\n",
    "for day in day_of_week_need_subsidy:\n",
    "    time_index_range_in_a_week_need_subsidy += list(arr([36,37,38,39,40]) * day)\n",
    "\n",
    "\n",
    "def t_func(t):\n",
    "    # the time state variable is an indicator for hours in week that need subsidy\n",
    "    return int(t % (48 * 7) in time_index_range_in_a_week_need_subsidy)\n",
    "range_transformed_time_index = [0, 1] # can be extended to more than two time slices\n",
    "\n",
    "location_give_subsidy = [0, 1, 2] # other regions will never receive subsidy\n",
    "\n",
    "# adaptive_tp_dictionary is a len-N of len-N_range_transformed_time_index of binary value indicating \n",
    "# whether or not this space-time slice should receive subsidy\n",
    "# For example: \n",
    "\n",
    "adaptive_action_list = []\n",
    "adaptive_tp_dictionary = []\n",
    "for region_index in range(9):\n",
    "    region_action=[]\n",
    "    for time_index in range_transformed_time_index:\n",
    "        if(time_index == 1 and region_index in location_give_subsidy):\n",
    "            adaptive_action_list.append(1)\n",
    "            region_action.append(1)\n",
    "        else:\n",
    "            adaptive_action_list.append(0)\n",
    "            region_action.append(0)\n",
    "    adaptive_tp_dictionary.append(region_action)\n",
    "\n",
    "print(adaptive_tp_dictionary)\n",
    "\n",
    "\n",
    "pi_adaptive = []\n",
    "for i in range(9):\n",
    "    def tp_i(s, a = 0, random_choose = False, fixed_policy_i = adaptive_tp_dictionary[i]): # , fixed_policy_i = adaptive_tp_dictionary[i]\n",
    "        t = int(s[3]) # already transformed by t_func\n",
    "        if random_choose:\n",
    "            return fixed_policy_i[t]\n",
    "        else:\n",
    "            return int(a == fixed_policy_i[t])\n",
    "    pi_adaptive.append(tp_i)\n",
    "target_policy = pi_adaptive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
