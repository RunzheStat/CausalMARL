\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage{neurips2020}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm,algpseudocode,enumerate}
\usepackage{bm}
\usepackage{xr}
\externaldocument{causalMARL}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\Mean}{{\mathbb{E}}}
\newcommand{\Var}{{\mbox{Var}}}
\newcommand{\Cov}{{\mbox{cov}}}
\newcommand{\Corr}{{\mbox{corr}}}
\newcommand{\diag}{{\mbox{diag}}}
\newcommand{\prob}{{\mathbb{P}}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\title{Supplement to ``Spatiotemporal Causal Effects Evaluation: A Multi-Agent Reinforcement Learning Framework"}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
	David S.~Hippocampus\thanks{Use footnote for providing further information
		about author (webpage, alternative address)---\emph{not} for acknowledging
		funding agencies.} \\
	Department of Computer Science\\
	Cranberry-Lemon University\\
	Pittsburgh, PA 15213 \\
	\texttt{hippo@cs.cranberry-lemon.edu} \\
	% examples of more authors
	% \And
	% Coauthor \\
	% Affiliation \\
	% Address \\
	% \texttt{email} \\
	% \AND
	% Coauthor \\
	% Affiliation \\
	% Address \\
	% \texttt{email} \\
	% \And
	% Coauthor \\
	% Affiliation \\
	% Address \\
	% \texttt{email} \\
	% \And
	% Coauthor \\
	% Affiliation \\
	% Address \\
	% \texttt{email} \\
}

\begin{document}
	
\maketitle

\appendix 
\section{More on the learning procedure}
\subsection{Estimation of the weight}\label{sec:weight}
Consider the following optimization problem
\begin{eqnarray}\label{optimize}
	\widehat{\omega}_i=\argmin_{\omega_i\in \Omega} \sup_{f\in \mathcal{F}} \left|\sum_{t=0}^{T-1} \Delta_{i,t}(\omega_i) f(S_{0,t+1},S_{i,t+1},\widetilde{S}_{i,t+1})\right|^2.
\end{eqnarray}
In our implementation, we set $\mathcal{F}$ to a unit ball of a reproducing kernel Hilbert space (RFHS), i.e., 
\begin{eqnarray*}
	\mathcal{F}=\{f\in \mathcal{H}:\|f\|_{\mathcal{H}}=1\},
\end{eqnarray*}
where $\mathcal{H}$ corresponds to an RFHS such that
\begin{eqnarray*}
	\mathcal{H}=\left\{f(\cdot)=\sum_{t=0}^{T-1} b_t \kappa(S_{0,t+1},S_{i,t+1},\widetilde{S}_{i,t+1};\cdot): \{b_t\}_{t=0}^{T-1}\in \mathbb{R}^{T} \right\},
\end{eqnarray*}
for some positive definite kernel $\kappa(\cdot;\cdot)$. Similar to Theorem 2 of \cite{liu2018}, the optimization problem in \eqref{optimize} is then reduced to 
\begin{eqnarray*}
	\widehat{\omega}_i=\argmin_{\omega_i\in \Omega} \sum_{t_1=0}^{T-1} \sum_{t_2=0}^{T-1} \Delta_{i,t_1}(\omega_i)\Delta_{i,t_2}(\omega_i) \kappa(S_{0,t_1+1},S_{i,t_1+1},\widetilde{S}_{i,t_1+1};S_{0,t_2+1},S_{i,t_2+1},\widetilde{S}_{i,t_2+1}). 
\end{eqnarray*}
We set $\Omega$ to be the class of neural networks. One could use different parameters to factorize different $\omega_i$'s such that each $\widehat{\omega}_i$ is computed separately. Alternatively, one could share some parameters in common to estimate $\omega_i$'s jointly. We detail our procedure in Algorithm \ref{alg1}. 

\begin{algorithm}[t!]
	\caption{Estimation of the weight.}
	\label{alg1}
	%\vspace*{-15pt}
	\begin{algorithmic}
		\item
		\begin{description}
			\item[\textbf{Input}:] The data $\{(S_{0,j},S_{i,j},A_{i,j},R_{i,j},S_{0,j+1},S_{i,j+1}):1\le i\le N,0\le j< T\}$. A target policy $\bm{\pi}$. 
			
			\item[\textbf{Initial}:] Initial the density ratio $\omega_i=\omega_{i,\theta}$ for $1\le i\le N$, to be neural networks parameterized by $\theta$.
			
			\item[\textbf{for}] iteration $=1,2,\cdots$ \textbf{do}
			\begin{enumerate}
				\item[a] Randomly sample a batch $\mathcal{M}$ from $\{0,1,\cdots,T-1\}$.
				
				\item[b] {\textbf{Update}} the parameter $\theta$ by $\theta\leftarrow \theta-\epsilon N^{-1}\sum_{i=1}^N \nabla_{\theta} D_i(\omega_{i,\theta}/z_{\omega_{i,\theta}})$ where $D_i(\omega_{i,\theta})$ is equal to
				\begin{eqnarray*}
					\frac{1}{|\mathcal{M}|}\sum_{t_1,t_2\in \mathcal{M}}  \Delta_{i,t_1}(\omega_{i,\theta})\Delta_{i,t_2}(\omega_{i,\theta}) \kappa(S_{0,t_1+1},S_{i,t_1+1},\widetilde{S}_{i,t_1+1};S_{0,t_2+1},S_{i,t_2+1},\widetilde{S}_{i,t_2+1}),
				\end{eqnarray*}
				and $z_{\omega_{i,\theta}}$ is a normalization constant $z_{\omega_{i,\theta}}=|\mathcal{M}|^{-1} \sum_{t\in \mathcal{M}} \omega_{i,\theta}(S_{0,t+1},S_{i,t+1},\widetilde{S}_{i,t+1})$. 
			\end{enumerate}
			\item[\textbf{Output}] $\omega_{i,\theta}$ for $1\le i\le N$. 
		\end{description}
	\end{algorithmic}
\end{algorithm}

\subsection{Estimation of the Q-function and the value}\label{sec:Q}
We now describe methods to estimate $Q_i$ and $V_i(\bm{\pi})$. For two given function classes $\mathcal{G}$ and $\mathcal{Q}$, define the following penalized estimator
\begin{eqnarray*}
	\widehat{g}_{i}(\cdot,\cdot,\cdot,\cdot,\cdot;\eta,Q_i)=\argmin_{g\in \mathcal{G}}\frac{1}{T}\sum_{t=0}^{T-1} \{R_{i,t}+Q_{i}(\pi_i,\widetilde{A}_i(\bm{\pi}),S_{0,t+1},S_{i,t+1},\widetilde{S}_{i,t+1})\\
	-\eta-Q_i(A_{i,t},\widetilde{A}_{i,t},S_{0,t},S_{i,t},\widetilde{S}_{i,t})-g(A_{i,t},\widetilde{A}_{i,t},S_{0,t},S_{i,t},\widetilde{S}_{i,t})\}^2+\mu J_2^2(g),\\
	(\widehat{V}_i(\bm{\pi}),\widehat{Q}_i)=\argmin_{(\eta,Q_i)\in \mathbb{R}\times \mathcal{Q}}\frac{1}{T}\sum_{t=0}^{T-1} \widehat{g}_{i}^2(A_{i,t},\widetilde{A}_{i,t},S_{0,t},S_{i,t},\widetilde{S}_{i,t};\eta,Q_i)+\lambda J_1^2(Q_i),
\end{eqnarray*}
where $J_1$ and $J_2$ denote some penalty functions, $\mu$ and $\lambda$ stand for some tuning parameters. Next we derive the close-form expressions of 	$(\widehat{V}_i(\bm{\pi}),\widehat{Q}_i)$ when using RKHS to model $Q_i$ and $g_i$. %The derivation is based on the results in the paper Farahmand et al. (2016). 

Define vectors $Z_{i,t}=(A_{i,t},\widetilde{A}_{i,t},S_{0,t},S_{i,t},\widetilde{S}_{i,t})^\top$ and $Z_{i,t}^*=(\pi_i,\widetilde{A}_i(\bm{\pi}),S_{0,t+1},S_{i,t+1},\widetilde{S}_{i,t+1})^\top$. Let $K_g$ and $K_Q$ denote the reproducing kernels used to model $g$ and $Q$, respectively. In practice, we can use gaussian RBF kernels to model these two functions. For a given $Q_i$ and $\eta$, the optimizer of $\widehat{g}_i$ can be represented by $\sum_{t=0}^{T-1} \widehat{\beta}_{i,t} K_g(Z_{i,t},\cdot)$. As such, we obtain
\begin{eqnarray*}
	\widehat{\bm{\beta}}_i=\argmin_{\bm{\beta}} \frac{1}{T}\sum_{t=0}^{T-1} \left\{R_{i,t}+Q_i(Z_{i,t}^*)-\eta-Q_i(Z_{i,t})-\sum_{j=0}^{T-1} \beta_{j} K_g(Z_{i,j},Z_{i,t}) \right\}^2+\mu \bm{\beta}^\top \bm{K}_g \bm{\beta}\\
	=\frac{1}{T} \bm{\beta}^\top \{\bm{K}_g\bm{K}_g^\top+T\mu \bm{K}_g\} \bm{\beta}-\frac{2}{T}\bm{\beta}^\top \bm{K}_g (\bm{R}+\bm{Q}_i^{*}-\bm{Q}_i-\eta \bm{1})
	+\hbox{some~terms~that~are~independent~of~}\bm{\beta},
\end{eqnarray*}
where $\bm{K}_g=\{K_g(Z_{i,j_1},Z_{i,j_2})\}_{j_1,j_2}$ and $\bm{R}$, $\bm{Q}_i^{*}$ and $\bm{Q}_i$ the column vectors formed by elements in $R_t$, $Q_i(Z_{i,t}^*)$ and $Q_i(Z_{i,t})$, respectively. Notice that $\bm{K}_g$ is symmetric, by some calculations, we obtain
\begin{eqnarray*}
	\widehat{\bm{\beta}}_i=(\bm{K}_g\bm{K}_g^\top+T\mu \bm{K}_g)^{-1} \bm{K}_g (\bm{R}+\bm{Q}_i^{*}-\bm{Q}_i-\eta \bm{1})
	=(\bm{K}_g+T\mu\bm{I})^{-1} (\bm{R}+\bm{Q}_i^{*}-\bm{Q}_i-\eta \bm{1}).
\end{eqnarray*}
As a result, for a given $Q_i$ and $\eta$, we have
\begin{eqnarray*}
	\widehat{g}_i(Z_{i,t};\eta,Q_i)=\widehat{\bm{\beta}}_i^\top \bm{K}_g \bm{e}_t,
\end{eqnarray*}
where $\bm{e}_t$ denotes the column vector with the $t$-th element equals to one and other elements equal to zero. As such,
\begin{eqnarray*}
	\frac{1}{T}\sum_{t=0}^{T-1} \widehat{g}_{i}^2(A_{i,t},\widetilde{A}_{i,t},S_{0,t},S_{i,t},\widetilde{S}_{i,t};\eta,Q_i)=\frac{1}{T}\widehat{\bm{\beta}}_i^\top \bm{K}_g \bm{K}_g^T \widehat{\bm{\beta}}_i.
\end{eqnarray*}
Similarly, we can represent $Q_i$ as $\sum_{t=0}^{2T-1} \widehat{\alpha}_{i,t} K_Q(\widetilde{Z}_{i,t},\cdot)$ where $\widetilde{Z}_{i,t}$ denotes the $t$-th element in the vector $(Z_{i,0}^\top,Z_{i,1}^\top,\cdots,Z_{i,T-1}^\top,Z_{i,0}^{*\top},\cdots,Z_{i,T-1}^{*\top})^\top$. Let $\bm{K}_Q$ denotes the corresponding $2T\times 2T$ matrix, we have
\begin{eqnarray*}
	Q_i(Z_{i,t})=\bm{\alpha}_i^\top \bm{K}_Q \bm{e}_t\,\,\,\,\hbox{and}\,\,\,\,Q_i(Z_{i,t}^*)=\widehat{\bm{\alpha}}_i^\top \bm{K}_Q \bm{e}_{t+T+1}.
\end{eqnarray*}
It follow that
\begin{eqnarray*}
	\bm{Q}_i^*-\bm{Q}_i=\underbrace{[-\bm{I}_{T}, \bm{I}_{T}]}_{\bm{C}}\bm{K}_Q \widehat{\bm{\alpha}}_i,
\end{eqnarray*}
noting that $\bm{K}_Q$ is symmetric. Let $\bm{E}=\bm{K}_g^\top (\bm{K}_g+T\mu \bm{I})^{-1}$, $\widehat{\bm{\alpha}}_i$ corresponds to the solution of the following optimization problem, 
\begin{eqnarray*}
	\widehat{\bm{\alpha}}_i=\argmin_{\bm{\alpha}} (\bm{R}+\bm{C} \bm{K}_Q \bm{\alpha}-\eta \bm{1})^\top \bm{E}^\top \bm{E} (\bm{R}+\bm{C} \bm{K}_Q \bm{\alpha}-\eta \bm{1})+T \lambda \bm{\alpha}^\top \bm{K}_Q \bm{\alpha}.
\end{eqnarray*}
Taking derivatives with respect to $\bm{\alpha}$ and $\eta$, we obtain
\begin{eqnarray*}
	(\widehat{\bm{\alpha}}_i,\widehat{V}_i(\bm{\pi}))^\top=-([\bm{C} \bm{K}_Q,-\bm{1}]^\top \bm{E}^\top \bm{E}[\bm{C} \bm{K}_Q,-\bm{1}]+[T\lambda \bm{K}_Q,\bm{0};\bm{0}^\top,0])^{-1}[\bm{C} \bm{K}_Q,-\bm{1}] \bm{E}^\top \bm{E} \bm{R}.
\end{eqnarray*}

\subsection{Estimation of the treatment assignment probability}\label{sec:propensity}
Note that $b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})=\Mean \{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))|S_{0,t},S_{i,t},\widetilde{S}_{i,t}\}$. It can thus be learned by applying machine learning algorithms to datasets with responses $\{ \mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi})):0\le t< T \}$ and predictors $\{(S_{0,t},S_{i,t},\widetilde{S}_{i,t}):0\le t<T\}$. 

\section{Additional technical conditions}\label{sec:addtechcond}

\section{Proofs}
We use $c$ and $C$ to denote some generic constants whose values are allowed to vary from place to place. 
Lemma \ref{lemma:weight} can thus be proven in a similar manner as Theorem 1 of \cite{liu2018}.  Lemma \ref{lemma:Q} can be similarly proven as Lemma 1 of \cite{shi2020reinforcement}. In the following, we focus on proving Theorems \ref{thm:oracle}, \ref{thm:double} and \ref{thm:oracleest}. 
\subsection{Proof of Theorem \ref{thm:oracle}}
To prove Theorem \ref{thm:oracle}, we apply the central limit theorem for mixing triangle arrays developed in \cite{francq2005central}. Define
\begin{eqnarray*}
	\widehat{V}_t^{\hbox{\scriptsize{DR}}*}(\bm{\pi})=\frac{1}{N}\sum_{i=1}^N \left[V_i^*(\bm{\pi})+ \omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}
	\{R_{i,t}+Q_{i,t+1}^*(\bm{\pi})-Q_{i,t}^*-V_i^*(\bm{\pi})\}\right],
\end{eqnarray*}
we have $\widehat{V}^{\hbox{\scriptsize{DR}}*}(\bm{\pi})=T^{-1} \sum_{t=0}^{T-1} \widehat{V}_t^{\hbox{\scriptsize{DR}}*}(\bm{\pi})$. 

Suppose we have shown each $\widehat{V}_t^{\hbox{\scriptsize{DR}}*}(\bm{\pi})$ is an unbiased estimator for $V(\bm{\pi})$. For $t\in \{0,1,\cdots,T-1\}$, let $x_t=(NT)^{-1/2} \{\widehat{V}_t^{\hbox{\scriptsize{DR}}*}(\bm{\pi})-V(\bm{\pi})\}$. It suffices to show the conditions in (1)-(5) of \cite{francq2005central} hold for $\{x_t:0\le t<T\}$. We next verify these conditions.

\textbf{Condition (1).} Note that $\{R_{i,t}, Q_i^*, \omega_{i}, V_i(\bm{\pi}):1\le i\le N,t\ge 0\}$ are uniformly bounded from infinity, the set of functions $\{b_i:1\le i\le N\}$ are uniformly bounded from zero. As such, $\{x_t:0\le t<T\}$ are uniformly bounded. Condition (1) thus holds for any $\nu^*>0$. 

\textbf{Condition (2).} This condition is automatically implied by the assumption that $ NT \Var\{\widehat{V}^{\hbox{\scriptsize{DR}}*}(\bm{\pi})\}\to \sigma^2>0$. 

\textbf{Condition (3).} This condition holds by setting $\kappa=0$ and $T_n=0$ for any $n$.

\textbf{Condition (4).} Note that the strong mixing coefficients are upper bounded by the $\beta$-mixing coefficients. Under Condition (A2), we can take the sequence $\alpha(h)$ in Condition (4) by $\kappa_0 \rho^h$. 

\textbf{Condition (5).} Since $\kappa_0 \rho^h$ decays to zero at an exponential rate as $h$ grows to infinity, Condition (5) is automatically satisfied. 

It remains to show $\Mean \widehat{V}_t^{\hbox{\scriptsize{DR}}*}(\bm{\pi})=V(\bm{\pi})$ for any $t$. Suppose (A4) holds. Under the given conditions, we have $V_i^*(\bm{\pi})=V_i(\bm{\pi})$. Under Lemma \ref{lemma:Q}, we have
\begin{eqnarray*}
	\Mean \{R_{i,t}+Q_{i,t+1}^*(\bm{\pi})-Q_{i,t}^*-V_i^*(\bm{\pi})|\bm{A}_t,\bm{S}_t\}=0,
\end{eqnarray*}
and hence,
\begin{eqnarray*}
	\Mean \omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}
	\{R_{i,t}+Q_{i,t+1}^*(\bm{\pi})-Q_{i,t}^*-V_i^*(\bm{\pi})\}=0.
\end{eqnarray*}
Consequently, $\Mean \widehat{V}_t^{\hbox{\scriptsize{DR}}*}(\bm{\pi})=N^{-1} \sum_{i=1}^N V_i(\bm{\pi})=V(\bm{\pi})$.

Suppose (A3)(i) holds. For any $i,t$, the expectation of the density ratio $\omega_{i,t}\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))/b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})$ equals one. As such, we have
\begin{eqnarray}\label{eqn:proofthm1}
\begin{split}
	\Mean \left\{V_i^*(\bm{\pi})- \omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}V_i^*(\bm{\pi})\right\}\\
	=V_i^*(\bm{\pi}) \Mean \left\{1- \omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})} \right\}=0.
\end{split}	
\end{eqnarray}
In addition, using similar arguments in \eqref{eqn:valueliu2018}, we have by (A3)(i) that 
\begin{eqnarray}\label{eqn:proofthm11}
	\Mean \left\{\omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}
	R_{i,t}\right\}=V_i(\bm{\pi}).
\end{eqnarray}
Moreover, by some calculations, we have
\begin{eqnarray*}
	\Mean \left\{\omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}Q_{i,t}^*\right\}=\Mean \left\{  \omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}Q_{i,t+1}^*(\bm{\pi})\right\}\\
	=\int_{s_0,s_i,\tilde{s}_i} Q_i^*(\pi_i,\widetilde{A}_i(\bm{\pi}),s_0,s_i,\tilde{s}_i)p(\bm{\pi},s_0,s_i,\tilde{s}_i)ds_0ds_id\tilde{s}_i.
\end{eqnarray*}
Consequently,
\begin{eqnarray*}
	\Mean \left[\omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}\{Q_{i,t+1}^*(\bm{\pi})-Q_{i,t}^*\}\right]=0.
\end{eqnarray*}
This together with \eqref{eqn:proofthm1} and \eqref{eqn:proofthm11} yields 
\begin{eqnarray*}
	\Mean \left[V_i^*(\bm{\pi})+ \omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}
	\{R_{i,t}+Q_{i,t+1}^*(\bm{\pi})-Q_{i,t}^*-V_i^*(\bm{\pi})\}\right]=V_i(\bm{\pi}). 
\end{eqnarray*}
It follows that $\Mean \widehat{V}^{\hbox{\scriptsize{DR}}*}(\bm{\pi})=V(\bm{\pi})$. 

Thus, $\widehat{V}^{\hbox{\scriptsize{DR}}*}(\bm{\pi})$ is unbiased when either (A3)(i) or (A4) holds. The proof is hence completed. 

\subsection{Proof of Theorem \ref{thm:double}}
By Theorem \ref{thm:oracle}, it suffices to show $\widehat{V}^{\hbox{\scriptsize{DR}}}(\bm{\pi})$ is asymptotically equivalent to $\widehat{V}^{\hbox{\scriptsize{DR}}*}(\bm{\pi})$. Note that $\widehat{V}^{\hbox{\scriptsize{DR}}}(\bm{\pi})-\widehat{V}^{\hbox{\scriptsize{DR}}*}(\bm{\pi})$ can be decomposed by $\eta_1+\eta_2+\eta_3+\eta_4+\eta_5$ where
\begin{eqnarray*}
	\eta_1&=&	\frac{1}{NT}\sum_{t=0}^{T-1}\sum_{i=1}^N \left\{\omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}-1\right\}\{V_i^*(\bm{\pi})- \widehat{V}_i(\bm{\pi}) \},\\
	\eta_2&=&%\frac{1}{T}\sum_{t=0}^{T-1} \{\widehat{V}_i(\bm{\pi})-V_i^*(\bm{\pi})\}+
	\frac{1}{NT}\sum_{t=0}^{T-1}\sum_{i=1}^N \omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}\{\widehat{Q}_{i,t+1}(\bm{\pi})-\widehat{Q}_{i,t}-Q_{i,t+1}^*(\bm{\pi})+Q_{i,t}^* \},\\
	\eta_3&=&\frac{1}{NT}\sum_{t=0}^{T-1}\sum_{i=1}^N (\widehat{\omega}_{i,t}-\omega_{i,t})\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}
	\{R_{i,t}+Q_{i,t+1}^*(\bm{\pi})-Q_{i,t}^*-V_i^*(\bm{\pi})\},\\
	\eta_4&=&\frac{1}{NT}\sum_{t=0}^{T-1}\sum_{i=1}^N (\widehat{\omega}_{i,t}-\omega_{i,t})\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}\{\widehat{Q}_{i,t+1}(\bm{\pi})-\widehat{Q}_{i,t}-Q_{i,t+1}^*(\bm{\pi})+Q_{i,t}^* \},\\
	\eta_5&=&\frac{1}{NT}\sum_{t=0}^{T-1}\sum_{i=1}^N (\widehat{\omega}_{i,t}-\omega_{i,t})\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}\{V_i^*(\bm{\pi})-\widehat{V}_i(\bm{\pi}) \}.
\end{eqnarray*}
In the following, we provide upper bounds on each $|\eta_j|$, for $j=1,2,\cdots,5$. 

\textbf{Upper bounds on $|\eta_1|$: }Note that $\eta_1=N^{-1}\sum_{i=1}^N \eta_{1,i}$ where
\begin{eqnarray*}
	\eta_{1,i}=\{V_i^*(\bm{\pi})- \widehat{V}_i(\bm{\pi}) \}\left[\frac{1}{T}\sum_{t=0}^{T-1} \left\{\omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}-1\right\}\right].
\end{eqnarray*}
The expectation of the density ratio equals one. As a result, we have
\begin{eqnarray*}
	\Mean \left\{\omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}-1\right\}=0,
\end{eqnarray*}
for any $i,t$. In the following, we apply the 

Under Condition (A2), the $\beta$-mixing coefficients of the sequence
\begin{eqnarray}\label{eqn:set}
	\left\{\omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}-1:t\ge 0\right\},
\end{eqnarray}
decays to zero at an exponential rate. In addition, all the terms in \eqref{eqn:set} are uniformly bounded. It follows from Corollary 3.3 in \cite{krebs2018} that
\begin{eqnarray*}
	\prob\left(\left|\sum_{t=0}^{T-1}  \left\{\omega_{i,t}\frac{\mathbb{I}(A_{i,t}=\pi_i,\widetilde{A}_{i,t}=\widetilde{A}_i(\bm{\pi}))}{b_i(\bm{\pi}|S_{0,t},S_{i,t},\widetilde{S}_{i,t})}-1\right\}\right|\ge \epsilon \right)\le c\exp\left(-\frac{CT\epsilon}{\log T \log \log T}\right),
\end{eqnarray*}
for some constants $c,C>0$. 


\textbf{Upper bounds on $|\eta_2|$: }

\textbf{Upper bounds on $|\eta_3|$: }

\textbf{Upper bounds on $|\eta_4|$ and $|\eta_5|$: }


\bibliography{CausalMARL}
\bibliographystyle{plain}
	
\end{document}
