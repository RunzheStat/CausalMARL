

--------------------------------------【03/05】                                             
 [lam, w_hidden] =  [0.001, 10]                                                    
MC-based mean [average reward] and its std: [5.962 0.084]                          
DR, IS, Susan, DR_NS                                                               
 bias: [0.169 3.364 0.43  0.065]                                                   
 std: [0.72  0.311 0.757 0.696]                                                    
 MSE: [0.74  3.378 0.871 0.699] std_MC: 0.084                                      
time spent until now: 11.8 mins                                                    
                                                                                   
                                                                                   
--------------------------------------                                             
 [lam, w_hidden] =  [0.001, 30]                                                    
MC-based mean [average reward] and its std: [5.962 0.084]                          
DR, IS, Susan, DR_NS                                                               
 bias: [0.156 3.243 0.43  0.037]                                                   
 std: [0.714 0.331 0.757 0.671]                                                    
 MSE: [0.731 3.26  0.871 0.672] std_MC: 0.084                                      
time spent until now: 24.0 mins                                                    
                                 

--------------------------------------
 [lam, w_hidden] =  [0.001, 50]
MC-based mean [average reward] and its std: [5.962 0.084]
DR, IS, Susan, DR_NS
 bias: [0.154 3.187 0.43  0.008]
 std: [0.715 0.35  0.757 0.655]
 MSE: [0.731 3.206 0.871 0.655] std_MC: 0.084
time spent until now: 36.2 mins


--------------------------------------
 [lam, w_hidden] =  [0.01, 10]
^[[A^[[A^[[AMC-based mean [average reward] and its std: [5.962 0.084]
DR, IS, Susan, DR_NS
 bias: [0.257 3.35  0.53  0.12 ]
 std: [0.786 0.316 0.833 0.74 ]
 MSE: [0.827 3.365 0.987 0.75 ] std_MC: 0.084
time spent until now: 48.2 mins


--------------------------------------
 [lam, w_hidden] =  [0.01, 30]
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[AMC-based mean [average reward] and its std: [5.962 0.084]
DR, IS, Susan, DR_NS
 bias: [0.251 3.25  0.53  0.084]
 std: [0.786 0.334 0.833 0.712]
 MSE: [0.825 3.267 0.987 0.717] std_MC: 0.084
time spent until now: 60.1 mins


--------------------------------------
 [lam, w_hidden] =  [0.01, 50]
MC-based mean [average reward] and its std: [5.962 0.084]
DR, IS, Susan, DR_NS
 bias: [0.247 3.191 0.53  0.053]
 std: [0.782 0.349 0.833 0.698]
 MSE: [0.82  3.21  0.987 0.7  ] std_MC: 0.084
time spent until now: 72.2 mins


--------------------------------------
 [lam, w_hidden] =  [0.1, 10]
MC-based mean [average reward] and its std: [5.962 0.084]
DR, IS, Susan, DR_NS
 bias: [0.308 3.358 0.556 0.181]
 std: [0.816 0.319 0.86  0.782]
 MSE: [0.872 3.373 1.024 0.803] std_MC: 0.084
time spent until now: 83.8 mins


--------------------------------------
 [lam, w_hidden] =  [0.1, 30]
MC-based mean [average reward] and its std: [5.962 0.084]
DR, IS, Susan, DR_NS
 bias: [0.3   3.249 0.556 0.129]
 std: [0.814 0.334 0.86  0.744]
 MSE: [0.868 3.266 1.024 0.755] std_MC: 0.084
time spent until now: 95.6 mins


--------------------------------------
 [lam, w_hidden] =  [0.1, 50]
MC-based mean [average reward] and its std: [5.962 0.084]
DR, IS, Susan, DR_NS
 bias: [0.296 3.187 0.556 0.086]
 std: [0.812 0.353 0.86  0.722]
 MSE: [0.864 3.206 1.024 0.727] std_MC: 0.084
time spent until now: 107.8 mins

==============================


[Non-stationary Driver]

l = 5
T = 14 * 48

a = now()
for lam in [0.01, 0.1]:
    for w_hidden in [10, 30, 50]:
        print(DASH, "[lam, w_hidden] = ", [lam, w_hidden])
        r = simu(pattern_seed = 0, OPE_rep_times = n_cores, l = l, T = T,  n_cores = n_cores, 
                  penalty = [lam, lam], reg_weight = 1e-4, 
                  w_hidden = w_hidden, 
                  batch_size = 128, 
                  max_iteration = 1001, epsilon = 5e-4, Learning_rate = 1e-4,
                  dim_S_plus_Ts = 3 + 3,
                  test_num = 0, inner_parallel = False)
        print( "time spent until now:", np.round((now() - a)/60, 1), "mins", "\n")



--------------------------------------
 [lam, w_hidden] =  [0.01, 10]
MC-based mean [average reward] and its std: [0.74  0.002]
DR, IS, Susan, DR_NS
 bias: [0.025 0.383 0.09  0.003]
 std: [0.02  0.012 0.017 0.022]
 std_MC: 0.002
time spent until now: 48.5 mins


--------------------------------------
 [lam, w_hidden] =  [0.01, 30]
^[[A^BdMC-based mean [average reward] and its std: [0.74  0.002]
DR, IS, Susan, DR_NS
 bias: [0.013 0.364 0.09  0.02 ]
 std: [0.02  0.013 0.017 0.024]
 std_MC: 0.002
time spent until now: 98.1 mins


--------------------------------------
 [lam, w_hidden] =  [0.01, 50]



=========================


# l = 3
# T = 100

# a = now()
# for lam in [0.01, 0.1]:
#     for w_hidden in [10, 30, 50]:


# MC-based mean [average reward] and its std: [1.102 0.019]                                                       
# DR, IS, Susan, DR_NS                                                                                            
#  bias: [0.062 0.56  0.007 0.164]                                                                                
#  std: [0.17  0.071 0.166 0.194]                                                                                 
#  std_MC: 0.019                                                                                                  
# time spent until now: 9.7 mins                                                                                  
                                                                                                                
# MC-based mean [average reward] and its std: [1.102 0.019]
# DR, IS, Susan, DR_NS
#  bias: [0.051 0.545 0.007 0.186]
#  std: [0.168 0.078 0.166 0.197]
#  std_MC: 0.019
# time spent until now: 19.9 mins


# MC-based mean [average reward] and its std: [1.102 0.019]
# DR, IS, Susan, DR_NS
#  bias: [0.044 0.54  0.007 0.194]
#  std: [0.165 0.08  0.166 0.199]
#  std_MC: 0.019
# time spent until now: 30.4 mins

# MC-based mean [average reward] and its std: [1.102 0.019]
# DR, IS, Susan, DR_NS
#  bias: [0.079 0.561 0.236 0.004]
#  std: [0.133 0.07  0.118 0.157]
#  std_MC: 0.019
# time spent until now: 40.0 mins


# MC-based mean [average reward] and its std: [1.102 0.019]
# DR, IS, Susan, DR_NS
#  bias: [0.086 0.544 0.236 0.033]
#  std: [0.134 0.078 0.118 0.161]
#  std_MC: 0.019
# time spent until now: 50.2 mins


# MC-based mean [average reward] and its std: [1.102 0.019]
# DR, IS, Susan, DR_NS
#  bias: [0.092 0.54  0.236 0.045]
#  std: [0.135 0.081 0.118 0.163]
#  std_MC: 0.019
# time spent until now: 60.7 mins